{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-pretrain-with-patent-data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoheikikuta/US-patent-analysis/blob/master/colab/BERT_pretrain_with_patent_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEkA6FkiZH9S",
        "colab_type": "text"
      },
      "source": [
        "# BERT pretraining with patent data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MZiSo7jZBbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mBWFwOZaGW",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpJxDRFBZMtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "6cd98ff0-1db8-4be0-a4a3-c54aa7dad646"
      },
      "source": [
        "!gsutil cp gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-info/citations_info_3000+3000.df.gz ./\n",
        "\n",
        "!gsutil cp gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/training_app_3000.df.gz ./  \n",
        "!gsutil cp gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/testset_app_3000.df.gz ./\n",
        "!gsutil cp gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/grants_for_3000+3000.df.gz ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-info/citations_info_3000+3000.df.gz...\n",
            "/ [1 files][506.5 KiB/506.5 KiB]                                                \n",
            "Operation completed over 1 objects/506.5 KiB.                                    \n",
            "Copying gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/training_app_3000.df.gz...\n",
            "| [1 files][ 45.0 MiB/ 45.0 MiB]                                                \n",
            "Operation completed over 1 objects/45.0 MiB.                                     \n",
            "Copying gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/testset_app_3000.df.gz...\n",
            "\\ [1 files][ 45.5 MiB/ 45.5 MiB]                                                \n",
            "Operation completed over 1 objects/45.5 MiB.                                     \n",
            "Copying gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-xml/grants_for_3000+3000.df.gz...\n",
            "- [1 files][129.4 MiB/129.4 MiB]                                                \n",
            "Operation completed over 1 objects/129.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghojum1vZhd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LANBzI0ZsCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "citations_info_target = pd.read_pickle(\"./citations_info_3000+3000.df.gz\")\n",
        "test_app = pd.read_pickle(\"./testset_app_3000.df.gz\")\n",
        "grants = pd.read_pickle(\"./grants_for_3000+3000.df.gz\")\n",
        "train_app = pd.read_pickle(\"./training_app_3000.df.gz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApvBSSuIZt4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "347ac640-1720-4190-8892-c600ec57c918"
      },
      "source": [
        "train_app.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>xml</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12130785</td>\n",
              "      <td>&lt;us-patent-application lang=\"EN\" dtd-version=\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12652424</td>\n",
              "      <td>&lt;us-patent-application lang=\"EN\" dtd-version=\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12214532</td>\n",
              "      <td>&lt;us-patent-application lang=\"EN\" dtd-version=\"...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     app_id                                                xml\n",
              "0  12130785  <us-patent-application lang=\"EN\" dtd-version=\"...\n",
              "1  12652424  <us-patent-application lang=\"EN\" dtd-version=\"...\n",
              "2  12214532  <us-patent-application lang=\"EN\" dtd-version=\"..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MgfIJcjZ5Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "CLAIM_PAT = re.compile(r'<claims[^>]*>(.*)</claims>',re.MULTILINE|re.DOTALL)\n",
        "TAG_PAT = re.compile(r\"<.*?>\")\n",
        "LB_PAT = re.compile(r'[\\t\\n\\r\\f\\v][\" \"]*')\n",
        "CANCELED_PAT = re.compile(r'[0-9]+.*\\. \\(canceled\\)[\" \"]')\n",
        "NUM_PAT = re.compile(r'[\" \"]?[0-9]+[\" \"]?\\.[\" \"]?')\n",
        "\n",
        "\n",
        "def whole_xml_to_claim_xml(whole):\n",
        "    mat = CLAIM_PAT.search(whole)\n",
        "    return mat.group(1)\n",
        "\n",
        "\n",
        "def whole_xml_to_claim(whole):\n",
        "    return TAG_PAT.sub(' ', whole_xml_to_claim_xml(whole))\n",
        "\n",
        "\n",
        "def remove_linebreak_from_claim(claim):\n",
        "    return LB_PAT.sub('', claim)\n",
        "\n",
        "\n",
        "def remove_canceled_claim(claim):\n",
        "    return CANCELED_PAT.sub('', claim)\n",
        "\n",
        "\n",
        "def remove_claim_numbers(claim):\n",
        "    return NUM_PAT.sub('', claim)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZ2bK80ag_9",
        "colab_type": "text"
      },
      "source": [
        "Test data will NOT be used for pretraining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibWuOQAFaYly",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5deb8e28-2d0e-45d2-cf36-345bed3e13e0"
      },
      "source": [
        "%%time\n",
        "\n",
        "train_app[\"claim_app\"] = train_app[\"xml\"].map(whole_xml_to_claim).map(remove_canceled_claim).map(remove_claim_numbers).map(remove_linebreak_from_claim)\n",
        "train_app = train_app.drop(\"xml\", axis=1)\n",
        "train_app.head()\n",
        "\n",
        "# test_app[\"claim_app\"] = test_app[\"xml\"].map(whole_xml_to_claim).map(remove_canceled_claim).map(remove_claim_numbers).map(remove_linebreak_from_claim)\n",
        "# test_app = test_app.drop(\"xml\", axis=1)\n",
        "# test_app.head()\n",
        "\n",
        "grants[\"claim_cited_grant\"] = grants[\"xml\"].map(whole_xml_to_claim).map(remove_canceled_claim).map(remove_claim_numbers).map(remove_linebreak_from_claim)\n",
        "grants = grants.drop(\"xml\", axis=1)\n",
        "grants.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.93 s, sys: 188 ms, total: 8.11 s\n",
            "Wall time: 8.12 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8oloFzVavKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9bb4a7a9-6342-4482-d050-9c5d15dfb18c"
      },
      "source": [
        "train_app.head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>app_id</th>\n",
              "      <th>claim_app</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12130785</td>\n",
              "      <td>A system for differentiating noise from an arr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12652424</td>\n",
              "      <td>A method of allocating resources in a data war...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12214532</td>\n",
              "      <td>A controlling method of a media processing app...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     app_id                                          claim_app\n",
              "0  12130785  A system for differentiating noise from an arr...\n",
              "1  12652424  A method of allocating resources in a data war...\n",
              "2  12214532  A controlling method of a media processing app..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEXJyiViav65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = train_app['claim_app'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwoWe3jta1pX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a3d2f32b-582c-4825-fbe9-d7e9a2e4d866"
      },
      "source": [
        "test.replace(\".\", \"\\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A system for differentiating noise from an arrhythmia of a heart, comprising:a noise discriminator configured to receive an electrocardiogram (EGM) signal and to discriminate between an organized EGM signal and a chaotic EGM signal based at least in part on an impedance parameter associated with a lead that provides an electrical connection to the heart; a signal analyzer configured to determine whether a chaotic signal is caused by a disturbance in the lead\\n The system of  claim 1 , further comprising a high voltage delivery system configured to deliver a high voltage therapy signal to the heart if the EGM signal is an organized signal\\n The system of  claim 2 , further comprising a high voltage confirmation system configured to adjust or terminate the high voltage therapy based on the impedance parameter\\n The system of  claim 3 , wherein the signal analyzer is part of the high voltage confirmation system\\n The system of  claim 3 , wherein the lead comprises a high voltage lead for delivering the high voltage signal to the heart\\n The system of  claim 3 , wherein the impedance parameter comprises an impedance value associated with an electrical connection of the lead with the heart\\n The system of  claim 3 , wherein the impedance parameter comprises an integrated value of impedance associated with an electrical connection of the lead with the heart\\n The system of  claim 1 , wherein the signal analyzer determines whether the chaotic signal is caused by lead disturbance by comparing the impedance parameter with a known threshold value\\n The system of  claim 8 , wherein the known threshold value comprises a threshold impedance value for the lead corresponding to a failure condition of the lead\\n The system of  claim 9 , wherein the failure condition of the lead and the corresponding threshold impedance value are determined by providing a simulated operating condition of the lead in a laboratory\\n An implantable cardiac device, comprising:a high voltage device configured to deliver a therapy signal to a heart when triggered; an electrical lead for connecting the high voltage device to the heart; an impedance measurement component configured to measure an electrical impedance associated with the electrical lead; and a processor configured to provide a command for operation of the high voltage device based at least in part on a parameter associated with the measured electrical impedance\\n The system of  claim 11 , wherein the parameter comprises an electrical resistance\\n The system of  claim 11 , wherein the parameter comprises an integrated value of electrical resistance over a period of time\\n The system of  claim 11 , wherein the processor provides the command based on comparison of the parameter with a known reference value\\n The system of  claim 14 , wherein the known reference value is stored in the implantable cardiac device, and obtained from a laboratory study that simulates degradation of the electrical lead\\n The system of  claim 15 , wherein the known reference value is obtained by correlating an observed failure condition with a corresponding value of the parameter\\n The system of  claim 11 , wherein the command comprises a termination command that terminates a process for delivering the therapy signal\\n The system of  claim 11 , wherein the command comprises an adjustment command that adjusts the therapy signal\\n A method for operating an implantable cardiac device, comprising:measuring an impedance value associated with at least one of a plurality of electrical leads for a high voltage device configured to provide a therapy signal, wherein the plurality of electrical leads are configured to be connected to a heart and deliver the therapy signal to the heart; and generating a command for operation of the high voltage device based at least in part on the measured impedance value\\n A method for differentiating noise from an arrhythmia of a heart, comprising:receiving an electrocardiogram (EGM) signal; measuring an impedance parameter associated with a lead that provides an electrical connection to the heart; and determining whether the EGM signal is an organized signal or a chaotic signal based at least in part on the measured impedance parameter\\n  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2GCTLbkdCTu",
        "colab_type": "text"
      },
      "source": [
        "In order to make a pretraining data, use simple preprocessing: replacing a (period + space) with (period + line break)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4O4uR5Hbb-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"./test.txt\", \"w+\") as f:\n",
        "    f.write(test.replace(\". \", \".\\n\").lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWOHlwl1cMom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "0d725615-7565-4e8c-8d49-bd2c770bde53"
      },
      "source": [
        "!cat ./test.txt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a system for differentiating noise from an arrhythmia of a heart, comprising:a noise discriminator configured to receive an electrocardiogram (egm) signal and to discriminate between an organized egm signal and a chaotic egm signal based at least in part on an impedance parameter associated with a lead that provides an electrical connection to the heart; a signal analyzer configured to determine whether a chaotic signal is caused by a disturbance in the lead.\n",
            "the system of  claim 1 , further comprising a high voltage delivery system configured to deliver a high voltage therapy signal to the heart if the egm signal is an organized signal.\n",
            "the system of  claim 2 , further comprising a high voltage confirmation system configured to adjust or terminate the high voltage therapy based on the impedance parameter.\n",
            "the system of  claim 3 , wherein the signal analyzer is part of the high voltage confirmation system.\n",
            "the system of  claim 3 , wherein the lead comprises a high voltage lead for delivering the high voltage signal to the heart.\n",
            "the system of  claim 3 , wherein the impedance parameter comprises an impedance value associated with an electrical connection of the lead with the heart.\n",
            "the system of  claim 3 , wherein the impedance parameter comprises an integrated value of impedance associated with an electrical connection of the lead with the heart.\n",
            "the system of  claim 1 , wherein the signal analyzer determines whether the chaotic signal is caused by lead disturbance by comparing the impedance parameter with a known threshold value.\n",
            "the system of  claim 8 , wherein the known threshold value comprises a threshold impedance value for the lead corresponding to a failure condition of the lead.\n",
            "the system of  claim 9 , wherein the failure condition of the lead and the corresponding threshold impedance value are determined by providing a simulated operating condition of the lead in a laboratory.\n",
            "an implantable cardiac device, comprising:a high voltage device configured to deliver a therapy signal to a heart when triggered; an electrical lead for connecting the high voltage device to the heart; an impedance measurement component configured to measure an electrical impedance associated with the electrical lead; and a processor configured to provide a command for operation of the high voltage device based at least in part on a parameter associated with the measured electrical impedance.\n",
            "the system of  claim 11 , wherein the parameter comprises an electrical resistance.\n",
            "the system of  claim 11 , wherein the parameter comprises an integrated value of electrical resistance over a period of time.\n",
            "the system of  claim 11 , wherein the processor provides the command based on comparison of the parameter with a known reference value.\n",
            "the system of  claim 14 , wherein the known reference value is stored in the implantable cardiac device, and obtained from a laboratory study that simulates degradation of the electrical lead.\n",
            "the system of  claim 15 , wherein the known reference value is obtained by correlating an observed failure condition with a corresponding value of the parameter.\n",
            "the system of  claim 11 , wherein the command comprises a termination command that terminates a process for delivering the therapy signal.\n",
            "the system of  claim 11 , wherein the command comprises an adjustment command that adjusts the therapy signal.\n",
            "a method for operating an implantable cardiac device, comprising:measuring an impedance value associated with at least one of a plurality of electrical leads for a high voltage device configured to provide a therapy signal, wherein the plurality of electrical leads are configured to be connected to a heart and deliver the therapy signal to the heart; and generating a command for operation of the high voltage device based at least in part on the measured impedance value.\n",
            "a method for differentiating noise from an arrhythmia of a heart, comprising:receiving an electrocardiogram (egm) signal; measuring an impedance parameter associated with a lead that provides an electrical connection to the heart; and determining whether the egm signal is an organized signal or a chaotic signal based at least in part on the measured impedance parameter.\n",
            " "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKg0gcucbr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7ff45cf-43e5-477e-9e71-ff0489a27a84"
      },
      "source": [
        "len(pd.concat([train_app['claim_app'], grants['claim_cited_grant']]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6WF30ogdy1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "86271ebe-40d8-43fc-dd89-4270323f3cb5"
      },
      "source": [
        "%%time\n",
        "\n",
        "with open(\"./training_data.txt\", \"w+\") as f:\n",
        "    for one_stuff in pd.concat([train_app['claim_app'], grants['claim_cited_grant']]):\n",
        "        f.write(one_stuff.replace(\". \", \".\\n\").lower())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 214 ms, sys: 101 ms, total: 316 ms\n",
            "Wall time: 325 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv1dGyvrev7R",
        "colab_type": "text"
      },
      "source": [
        "## Create training data for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKBYvybOeeM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07d9505f-4327-402b-c7dc-57d5b7b6fabd"
      },
      "source": [
        "!git clone https://github.com/google-research/bert.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a96WSrtefUKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "9f295ee5-cdae-47b0-b18e-d6f74c64be3f"
      },
      "source": [
        "!gsutil cp -r gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12 ./"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/bert_config.json...\n",
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n",
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n",
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n",
            "\\ [4 files][420.9 MiB/420.9 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/checkpoint...\n",
            "Copying gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/vocab.txt...\n",
            "| [6 files][421.1 MiB/421.1 MiB]                                                \n",
            "Operation completed over 6 objects/421.1 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SbZCuTAfg23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "78ae4db8-dc53-4099-b97b-c76f97461674"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\t\t\ttestset_app_3000.df.gz\n",
            "bert\t\t\t\ttest.txt\n",
            "citations_info_3000+3000.df.gz\ttraining_app_3000.df.gz\n",
            "grants_for_3000+3000.df.gz\ttraining_data.txt\n",
            "sample_data\t\t\tuncased_L-12_H-768_A-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m9L8nSlfsWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93e610a8-7922-420d-c347-f6b9a84848f8"
      },
      "source": [
        "%cd bert"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jm9alRxgObM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "c5e6e9e9-6313-45dd-9034-9a288cc48c17"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONTRIBUTING.md\t\t    predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
            "create_pretraining_data.py  README.md\n",
            "extract_features.py\t    requirements.txt\n",
            "__init__.py\t\t    run_classifier.py\n",
            "LICENSE\t\t\t    run_classifier_with_tfhub.py\n",
            "modeling.py\t\t    run_pretraining.py\n",
            "modeling_test.py\t    run_squad.py\n",
            "multilingual.md\t\t    sample_text.txt\n",
            "optimization.py\t\t    tokenization.py\n",
            "optimization_test.py\t    tokenization_test.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-j1rUNugPQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2985559-1578-4a40-9049-f64b73ffa030"
      },
      "source": [
        "%%time\n",
        "\n",
        "!python create_pretraining_data.py \\\n",
        "  --input_file=../training_data.txt \\\n",
        "  --output_file=../training_data.tfrecord \\\n",
        "  --vocab_file=../uncased_L-12_H-768_A-12/vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=5"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 11:48:29.829970 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0811 11:48:29.830970 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0811 11:48:29.831152 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0811 11:48:29.831310 140302422521728 deprecation_wrapper.py:119] From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0811 11:48:29.946908 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0811 11:48:29.953678 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0811 11:48:29.953933 140302422521728 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0811 11:48:29.954017 140302422521728 create_pretraining_data.py:448]   ../training_data.txt\n",
            "I0811 12:00:06.218663 140302422521728 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0811 12:00:06.218976 140302422521728 create_pretraining_data.py:459]   ../training_data.tfrecord\n",
            "W0811 12:00:06.219201 140302422521728 deprecation_wrapper.py:119] From create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0811 12:00:06.220709 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.221114 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the system of claim 27 , wherein the second portion is further adapted for determining that the packet data includes at least one of the threat signatures from the sub set [MASK] [SEP] system of claim 28 , wherein the second portion is further adapted for determining that the packet data includes at least one of the threat signatures from the common set . the system of claim 23 , wherein the third portion is further adapted for determining that the ##kka packet is associated with a [MASK] port scan based on the comparison of the first port weight ##ing to the default port weight ##ing . the system of claim 23 , wherein the fourth portion is further adapted for determining that the network packet is associated with a denial of service [MASK] based on the comparison of the first source weight ##ing to the default source weight ##ing . the system of claim 23 [MASK] wherein the fifth portion is further adapted for determining that [MASK] packet data fails to demonstrate that the remote source has a re ##quisite permission for connection to the network resource . the system of claim 23 , wherein the at least one of the plurality of actions comprises alert ##ing a network management system . the system of claim 23 , wherein the at least one of the plurality of [MASK] comprises rejecting the network packet . the system of claim 23 , wherein the at least one of the plurality of actions comprises blocking network traffic from the remote source . the system of claim 23 , wherein the at least one of the plurality of [MASK] comprises filtering port scan activity . the system of claim 23 , wherein the at least [MASK] of the plurality of actions comprises th ##rot ##tling network traffic to the network resource . the system of claim 23 , wherein the at least one of the plurality of actions comprises th ##rot ##tling network traffic received from to the [MASK] source . the system of claim 23 , wherein the at least one of the plurality of actions comprises di ##sa ##bling [MASK] network port associated with the network resource . the system of claim 23 , further comprising : an eighth portion adapted for generating an ac ##k [MASK] ##led ##gm ##ent message to the remote source , determining that the remote source failed to [MASK] a confirmation message , and determining that the network packet is associated with an existing network threat based on [MASK] determining that the remote source failed to [MASK] a confirmation message . the system of claim 23 , further comprising : a ninth portion adapted for comparing the packet data to a first behavior model , and determining that the network packet is associated with an existing network threat based on the comparison of the packet data to the first behavior model . the system of [MASK] 41 [MASK] wherein the first behavior model is repeatedly updated at a [MASK] ##de ##ter ##mined interval in run ##time [SEP]\n",
            "I0811 12:00:06.221384 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 2291 1997 4366 2676 1010 16726 1996 2117 4664 2003 2582 5967 2005 12515 2008 1996 14771 2951 2950 2012 2560 2028 1997 1996 5081 16442 2013 1996 4942 2275 103 102 2291 1997 4366 2654 1010 16726 1996 2117 4664 2003 2582 5967 2005 12515 2008 1996 14771 2951 2950 2012 2560 2028 1997 1996 5081 16442 2013 1996 2691 2275 1012 1996 2291 1997 4366 2603 1010 16726 1996 2353 4664 2003 2582 5967 2005 12515 2008 1996 15714 14771 2003 3378 2007 1037 103 3417 13594 2241 2006 1996 7831 1997 1996 2034 3417 3635 2075 2000 1996 12398 3417 3635 2075 1012 1996 2291 1997 4366 2603 1010 16726 1996 2959 4664 2003 2582 5967 2005 12515 2008 1996 2897 14771 2003 3378 2007 1037 14920 1997 2326 103 2241 2006 1996 7831 1997 1996 2034 3120 3635 2075 2000 1996 12398 3120 3635 2075 1012 1996 2291 1997 4366 2603 103 16726 1996 3587 4664 2003 2582 5967 2005 12515 2008 103 14771 2951 11896 2000 10580 2008 1996 6556 3120 2038 1037 2128 24871 6656 2005 4434 2000 1996 2897 7692 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 4506 8681 9499 2075 1037 2897 2968 2291 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 103 8681 21936 1996 2897 14771 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 4506 8681 10851 2897 4026 2013 1996 6556 3120 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 103 8681 22910 3417 13594 4023 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 103 1997 1996 29018 1997 4506 8681 16215 21709 15073 2897 4026 2000 1996 2897 7692 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 4506 8681 16215 21709 15073 2897 4026 2363 2013 2000 1996 103 3120 1012 1996 2291 1997 4366 2603 1010 16726 1996 2012 2560 2028 1997 1996 29018 1997 4506 8681 4487 3736 9709 103 2897 3417 3378 2007 1996 2897 7692 1012 1996 2291 1997 4366 2603 1010 2582 9605 1024 2019 5964 4664 5967 2005 11717 2019 9353 2243 103 3709 21693 4765 4471 2000 1996 6556 3120 1010 12515 2008 1996 6556 3120 3478 2000 103 1037 13964 4471 1010 1998 12515 2008 1996 2897 14771 2003 3378 2007 2019 4493 2897 5081 2241 2006 103 12515 2008 1996 6556 3120 3478 2000 103 1037 13964 4471 1012 1996 2291 1997 4366 2603 1010 2582 9605 1024 1037 6619 4664 5967 2005 13599 1996 14771 2951 2000 1037 2034 5248 2944 1010 1998 12515 2008 1996 2897 14771 2003 3378 2007 2019 4493 2897 5081 2241 2006 1996 7831 1997 1996 14771 2951 2000 1996 2034 5248 2944 1012 1996 2291 1997 103 4601 103 16726 1996 2034 5248 2944 2003 8385 7172 2012 1037 103 3207 3334 25089 13483 1999 2448 7292 102\n",
            "I0811 12:00:06.221601 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.221826 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.221915 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 32 82 88 98 134 157 168 214 229 276 293 336 359 386 403 423 431 490 492 503\n",
            "I0811 12:00:06.221997 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1012 2897 2034 3417 2886 1010 1996 1996 4506 4506 2028 6556 1037 19779 2709 1996 2709 4366 1010 3653\n",
            "I0811 12:00:06.222091 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.222164 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.223031 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.223346 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the beam combine ##r according to claim 30 , wherein the first [MASK] beam can pass through the ac ##ous ##to - optical element such that the mechanical wave or sound wave does not def ##le ##ct or bend [MASK] first light [MASK] and that the mechanical wave or sound wave def ##le ##cts or bends the at least one second light beam such that the first light beam and the at least one second light beam exit the ac ##ous ##to - optical element in a collin ##ear manner as a combined light beam . [SEP] . the method according to claim 2 , further comprising filtering the selected event ##ios on at least one geographical location . the method according to nemesis 1 , further comprising the step of identifying each of the [MASK] of indications and warnings . the method according to claim 1 , further comprising the step of ##ide ##nti ##fying the at least one parameter for each of the plurality of indications and warnings [MASK] the [MASK] according to claim 1 , further comprising the step of crawling the at least one information source to identify the downloadable data [MASK] the method according to claim 1 , wherein the step of download ##ing the data comprises the steps of : download ##ing the data from the at least one information source ; and storing the data in a first database . the method according to claim 8 , further comprising the step of encoding the downloaded data . the method according to claim 1 , further comprising the step of storing a ur ##l kicking with the at least one information source in a second database . the method according to claim 1 , further comprising the [MASK] of translating the data from a first language into a second language . the method [MASK] to claim 1 , wherein the step of [MASK] comprises the steps of : receiving at the first server at least one key ##word ; comparing the at least one key ##word to the at least one parameter ; and output ##ting a list comprising the subset of the data . the method according to claim 1 , further comprising the steps of : storing a filter event schedule having at least one time event indexed to 24 pre - determined time of day ; and executing the time event at the pre ##de ##ter ##mined time of day . the method according to claim 1 , wherein the first scale value comprises one of a number , a word , a phrase , and a color . the method according to claim 14 , wherein the word is one [MASK] & # x ##20 ##1 ##c ; alert , & # x ##20 ##1 ##d ; & # x ##20 ##1 ##c ; watch , & # x ##20 ##1 ##d ; and & # x [MASK] ##1 ##c [MASK] warning . & # x ##20 ##1 ##d ; the method according to claim 14 , wherein the [MASK] [SEP]\n",
            "I0811 12:00:06.223580 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 7504 11506 2099 2429 2000 4366 2382 1010 16726 1996 2034 103 7504 2064 3413 2083 1996 9353 3560 3406 1011 9380 5783 2107 2008 1996 6228 4400 2030 2614 4400 2515 2025 13366 2571 6593 2030 8815 103 2034 2422 103 1998 2008 1996 6228 4400 2030 2614 4400 13366 2571 16649 2030 23394 1996 2012 2560 2028 2117 2422 7504 2107 2008 1996 2034 2422 7504 1998 1996 2012 2560 2028 2117 2422 7504 6164 1996 9353 3560 3406 1011 9380 5783 1999 1037 22180 14644 5450 2004 1037 4117 2422 7504 1012 102 1012 1996 4118 2429 2000 4366 1016 1010 2582 9605 22910 1996 3479 2724 10735 2006 2012 2560 2028 10056 3295 1012 1996 4118 2429 2000 21363 1015 1010 2582 9605 1996 3357 1997 12151 2169 1997 1996 103 1997 24936 1998 16234 1012 1996 4118 2429 2000 4366 1015 1010 2582 9605 1996 3357 1997 5178 16778 14116 1996 2012 2560 2028 16381 2005 2169 1997 1996 29018 1997 24936 1998 16234 103 1996 103 2429 2000 4366 1015 1010 2582 9605 1996 3357 1997 15927 1996 2012 2560 2028 2592 3120 2000 6709 1996 26720 2951 103 1996 4118 2429 2000 4366 1015 1010 16726 1996 3357 1997 8816 2075 1996 2951 8681 1996 4084 1997 1024 8816 2075 1996 2951 2013 1996 2012 2560 2028 2592 3120 1025 1998 23977 1996 2951 1999 1037 2034 7809 1012 1996 4118 2429 2000 4366 1022 1010 2582 9605 1996 3357 1997 17181 1996 22817 2951 1012 1996 4118 2429 2000 4366 1015 1010 2582 9605 1996 3357 1997 23977 1037 24471 2140 10209 2007 1996 2012 2560 2028 2592 3120 1999 1037 2117 7809 1012 1996 4118 2429 2000 4366 1015 1010 2582 9605 1996 103 1997 22969 1996 2951 2013 1037 2034 2653 2046 1037 2117 2653 1012 1996 4118 103 2000 4366 1015 1010 16726 1996 3357 1997 103 8681 1996 4084 1997 1024 4909 2012 1996 2034 8241 2012 2560 2028 3145 18351 1025 13599 1996 2012 2560 2028 3145 18351 2000 1996 2012 2560 2028 16381 1025 1998 6434 3436 1037 2862 9605 1996 16745 1997 1996 2951 1012 1996 4118 2429 2000 4366 1015 1010 2582 9605 1996 4084 1997 1024 23977 1037 11307 2724 6134 2383 2012 2560 2028 2051 2724 25331 2000 2484 3653 1011 4340 2051 1997 2154 1025 1998 23448 1996 2051 2724 2012 1996 3653 3207 3334 25089 2051 1997 2154 1012 1996 4118 2429 2000 4366 1015 1010 16726 1996 2034 4094 3643 8681 2028 1997 1037 2193 1010 1037 2773 1010 1037 7655 1010 1998 1037 3609 1012 1996 4118 2429 2000 4366 2403 1010 16726 1996 2773 2003 2028 103 1004 1001 1060 11387 2487 2278 1025 9499 1010 1004 1001 1060 11387 2487 2094 1025 1004 1001 1060 11387 2487 2278 1025 3422 1010 1004 1001 1060 11387 2487 2094 1025 1998 1004 1001 1060 103 2487 2278 103 5432 1012 1004 1001 1060 11387 2487 2094 1025 1996 4118 2429 2000 4366 2403 1010 16726 1996 103 102\n",
            "I0811 12:00:06.223820 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.261949 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.262268 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 13 40 43 112 124 136 171 173 196 216 271 294 310 319 346 388 451 488 491 510\n",
            "I0811 12:00:06.262399 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 2422 1996 7504 2241 4366 29018 1012 4118 1012 1024 3378 3357 2429 22910 2560 1037 1997 11387 1025 3609\n",
            "I0811 12:00:06.262531 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.262635 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.264059 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.264444 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the cap according to claim 20 , wherein the at least two different characteristics comprise at least a difference angelica at least one of size , shape and weight . [SEP] [MASK] [MASK] according to claim 1 , wherein the at least one component comprises a plurality of components having at least two different characteristics , and the at least two different characteristics comprise at least a difference in at least one of size , shape and weight . the cap according to claim 1 , wherein the lid is dome shaped . the cap according to claim 1 , wherein the [MASK] body of one piece construction further includes a portion for attach ##ing the cap body to a container so that the at least one component can flow into the container in the activated position . in combination , the cap according to claim 24 with a container . the combination according to claim 25 , wherein the container is formed of a single wall material . the cap according to claim 24 , wherein the portion for [MASK] ##ing the cap body to a container comprises a threaded portion . the cap according to claim 27 , further including at least one vent in the threaded portion . the cap according to claim 1 , wherein the plunge ##r is mounted [MASK] vertical movement between the storage position and the activated position . the cap according to claim 1 , further [MASK] elements to prevent rotation of the plunge ##r . the cap according to claim 1 , wherein the cap body is of rigid construction . the cap according to claim 13 , wherein the plunge ##r includes a wall surrounding the [MASK] portion , and the wall contacts the periphery of the di ##sp ##ens ##ing tip to break to the seal . the [MASK] according to claim 1 , wherein the di ##sp ##ens ##ing tip and the cap body comprise one piece ; the cap body comprising a wall surrounding a hollow portion forming at least a portion of the chamber ; a peripheral portion of the di ##sp ##ens ##ing tip being attached to a lower portion of the wall forming a lower portion of the chamber ; and the plunge ##r being mo ##vable between the storage position and the activated position to break [MASK] seal formed by the di ##sp ##ens ##ing tip and the cap body [MASK] an entirety of the peripheral portion . the cap according to claim 33 , wherein , in the activated position , the plunge ##r , the cap body and the di ##sp ##ens ##ing tip comprise an integral [MASK] . the cap according to claim [MASK] , wherein the dome jaya shaped lid abu ##ts against an upper surface of filming cap body in the activated position . the cap according [MASK] claim 1 , further including a di ##sp ##ens ##ing member . [MASK] cap according to claim 1 , wherein the seal comprises an ann ##ular snap ring seal . [SEP]\n",
            "I0811 12:00:06.264721 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 6178 2429 2000 4366 2322 1010 16726 1996 2012 2560 2048 2367 6459 15821 2012 2560 1037 4489 19458 2012 2560 2028 1997 2946 1010 4338 1998 3635 1012 102 103 103 2429 2000 4366 1015 1010 16726 1996 2012 2560 2028 6922 8681 1037 29018 1997 6177 2383 2012 2560 2048 2367 6459 1010 1998 1996 2012 2560 2048 2367 6459 15821 2012 2560 1037 4489 1999 2012 2560 2028 1997 2946 1010 4338 1998 3635 1012 1996 6178 2429 2000 4366 1015 1010 16726 1996 11876 2003 8514 5044 1012 1996 6178 2429 2000 4366 1015 1010 16726 1996 103 2303 1997 2028 3538 2810 2582 2950 1037 4664 2005 22476 2075 1996 6178 2303 2000 1037 11661 2061 2008 1996 2012 2560 2028 6922 2064 4834 2046 1996 11661 1999 1996 8878 2597 1012 1999 5257 1010 1996 6178 2429 2000 4366 2484 2007 1037 11661 1012 1996 5257 2429 2000 4366 2423 1010 16726 1996 11661 2003 2719 1997 1037 2309 2813 3430 1012 1996 6178 2429 2000 4366 2484 1010 16726 1996 4664 2005 103 2075 1996 6178 2303 2000 1037 11661 8681 1037 26583 4664 1012 1996 6178 2429 2000 4366 2676 1010 2582 2164 2012 2560 2028 18834 1999 1996 26583 4664 1012 1996 6178 2429 2000 4366 1015 1010 16726 1996 25912 2099 2003 5614 103 7471 2929 2090 1996 5527 2597 1998 1996 8878 2597 1012 1996 6178 2429 2000 4366 1015 1010 2582 103 3787 2000 4652 9963 1997 1996 25912 2099 1012 1996 6178 2429 2000 4366 1015 1010 16726 1996 6178 2303 2003 1997 11841 2810 1012 1996 6178 2429 2000 4366 2410 1010 16726 1996 25912 2099 2950 1037 2813 4193 1996 103 4664 1010 1998 1996 2813 10402 1996 23275 1997 1996 4487 13102 6132 2075 5955 2000 3338 2000 1996 7744 1012 1996 103 2429 2000 4366 1015 1010 16726 1996 4487 13102 6132 2075 5955 1998 1996 6178 2303 15821 2028 3538 1025 1996 6178 2303 9605 1037 2813 4193 1037 8892 4664 5716 2012 2560 1037 4664 1997 1996 4574 1025 1037 15965 4664 1997 1996 4487 13102 6132 2075 5955 2108 4987 2000 1037 2896 4664 1997 1996 2813 5716 1037 2896 4664 1997 1996 4574 1025 1998 1996 25912 2099 2108 9587 12423 2090 1996 5527 2597 1998 1996 8878 2597 2000 3338 103 7744 2719 2011 1996 4487 13102 6132 2075 5955 1998 1996 6178 2303 103 2019 15700 1997 1996 15965 4664 1012 1996 6178 2429 2000 4366 3943 1010 16726 1010 1999 1996 8878 2597 1010 1996 25912 2099 1010 1996 6178 2303 1998 1996 4487 13102 6132 2075 5955 15821 2019 9897 103 1012 1996 6178 2429 2000 4366 103 1010 16726 1996 8514 24120 5044 11876 8273 3215 2114 2019 3356 3302 1997 7467 6178 2303 1999 1996 8878 2597 1012 1996 6178 2429 103 4366 1015 1010 2582 2164 1037 4487 13102 6132 2075 2266 1012 103 6178 2429 2000 4366 1015 1010 16726 1996 7744 8681 2019 5754 7934 10245 3614 7744 1012 102\n",
            "I0811 12:00:06.264947 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.265157 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.265244 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 20 32 33 49 103 157 181 222 225 245 287 310 394 408 447 454 459 469 480 493\n",
            "I0811 12:00:06.265325 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1999 1996 6178 6177 6178 2423 22476 2099 2005 2164 8892 6178 1996 2247 3131 2603 1011 1996 2000 1996\n",
            "I0811 12:00:06.265415 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.265490 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.266367 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.266705 140302422521728 create_pretraining_data.py:151] tokens: [CLS] having an [MASK] ##pressed condition in which no force [MASK] ex ##erted upon the key by a user and a pressed condition in which force is ex ##erted on the key [MASK] a user ; a force detection circuit configured to ##scan each key of the plurality [MASK] determine if a scanned key is in a pressed condition , quan ##tify , upon determining that a scanned key is in a [MASK] condition , the force ex ##erted by a user on said key determined to be in a pressed condition , and proceed , upon determining that a scanned [MASK] is not in a pressed condition , to another key of said plurality without quan ##tify ##ing a force ex ##erted on the key determined not to be in a pressed condition ; a first group of conductors ; a second group of conductors positioned in close ##吉 to the first group of conductors , the first and second groups prince conductors forming a plurality of intersections between first group conductors and second group conductors ; and a force sensitive resist ##ive element located between the first group conductor and the second group conductor of each [MASK] the plurality of intersections , wherein ##ea ##ch of the plurality of intersections [MASK] to an associated key of the plurality of keys , each of the associated keys configured to com ##press the resist ##ive element located at the [MASK] intersection upon ex ##ert ##ion of force on the associated key , the force detection circuit is configured to quan ##tify force ex ##erted upon each of the associated keys based upon changes in resistance value of the resist ##ive element at each corresponding intersection , the force detection circuit comprises a micro ##pro ##ces ##sor and a voltage divide ##r , [MASK] the micro ##pro ##ces ##sor is configured to place the voltage divide ##r in a first condition when scanning each key of the plurality to determine if a scanned key is in a pressed condition and in a second condition when quan [SEP] the computer keyboard of claim 1 , wherein the plurality of keys includes multiple character keys having respective characters assigned there ##to and a plurality of mod ##ifier keys , and wherein the micro ##pro ##ces ##sor is configured to generate a first signal upon detecting a character key to be in a pressed condition and to generate a papacy signal [MASK] detecting said character key and a mod ##ifier key to simultaneously be in a pressed condition . the computer keyboard of claim 1 , wherein : the voltage divide ##r includes a voltage measuring node , when the force detection circuit is in the first condition , voltage at the [MASK] measuring node varies within a first range as a key is pressed , when the [MASK] detection [MASK] is in the second condition , voltage at the voltage measuring node varies within a second range as [MASK] key is pressed , and the second range is larger than the first range . [SEP]\n",
            "I0811 12:00:06.266962 140302422521728 create_pretraining_data.py:161] input_ids: 101 2383 2019 103 19811 4650 1999 2029 2053 2486 103 4654 28728 2588 1996 3145 2011 1037 5310 1998 1037 4508 4650 1999 2029 2486 2003 4654 28728 2006 1996 3145 103 1037 5310 1025 1037 2486 10788 4984 26928 2000 29378 2169 3145 1997 1996 29018 103 5646 2065 1037 11728 3145 2003 1999 1037 4508 4650 1010 24110 27351 1010 2588 12515 2008 1037 11728 3145 2003 1999 1037 103 4650 1010 1996 2486 4654 28728 2011 1037 5310 2006 2056 3145 4340 2000 2022 1999 1037 4508 4650 1010 1998 10838 1010 2588 12515 2008 1037 11728 103 2003 2025 1999 1037 4508 4650 1010 2000 2178 3145 1997 2056 29018 2302 24110 27351 2075 1037 2486 4654 28728 2006 1996 3145 4340 2025 2000 2022 1999 1037 4508 4650 1025 1037 2034 2177 1997 23396 1025 1037 2117 2177 1997 23396 10959 1999 2485 30319 2000 1996 2034 2177 1997 23396 1010 1996 2034 1998 2117 2967 3159 23396 5716 1037 29018 1997 26540 2090 2034 2177 23396 1998 2117 2177 23396 1025 1998 1037 2486 7591 9507 3512 5783 2284 2090 1996 2034 2177 7589 1998 1996 2117 2177 7589 1997 2169 103 1996 29018 1997 26540 1010 16726 5243 2818 1997 1996 29018 1997 26540 103 2000 2019 3378 3145 1997 1996 29018 1997 6309 1010 2169 1997 1996 3378 6309 26928 2000 4012 20110 1996 9507 3512 5783 2284 2012 1996 103 6840 2588 4654 8743 3258 1997 2486 2006 1996 3378 3145 1010 1996 2486 10788 4984 2003 26928 2000 24110 27351 2486 4654 28728 2588 2169 1997 1996 3378 6309 2241 2588 3431 1999 5012 3643 1997 1996 9507 3512 5783 2012 2169 7978 6840 1010 1996 2486 10788 4984 8681 1037 12702 21572 9623 21748 1998 1037 10004 11443 2099 1010 103 1996 12702 21572 9623 21748 2003 26928 2000 2173 1996 10004 11443 2099 1999 1037 2034 4650 2043 13722 2169 3145 1997 1996 29018 2000 5646 2065 1037 11728 3145 2003 1999 1037 4508 4650 1998 1999 1037 2117 4650 2043 24110 102 1996 3274 9019 1997 4366 1015 1010 16726 1996 29018 1997 6309 2950 3674 2839 6309 2383 7972 3494 4137 2045 3406 1998 1037 29018 1997 16913 18095 6309 1010 1998 16726 1996 12702 21572 9623 21748 2003 26928 2000 9699 1037 2034 4742 2588 25952 1037 2839 3145 2000 2022 1999 1037 4508 4650 1998 2000 9699 1037 25097 4742 103 25952 2056 2839 3145 1998 1037 16913 18095 3145 2000 7453 2022 1999 1037 4508 4650 1012 1996 3274 9019 1997 4366 1015 1010 16726 1024 1996 10004 11443 2099 2950 1037 10004 9854 13045 1010 2043 1996 2486 10788 4984 2003 1999 1996 2034 4650 1010 10004 2012 1996 103 9854 13045 9783 2306 1037 2034 2846 2004 1037 3145 2003 4508 1010 2043 1996 103 10788 103 2003 1999 1996 2117 4650 1010 10004 2012 1996 10004 9854 13045 9783 2306 1037 2117 2846 2004 103 3145 2003 4508 1010 1998 1996 2117 2846 2003 3469 2084 1996 2034 2846 1012 102\n",
            "I0811 12:00:06.267179 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.365151 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.365475 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 3 10 32 48 72 101 149 162 177 198 212 239 302 405 407 458 460 474 476 495\n",
            "I0811 12:00:06.365630 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 4895 2003 2011 2000 4508 3145 10039 1997 1025 1997 14788 7978 1998 2367 2588 10004 13045 2486 4984 1037\n",
            "I0811 12:00:06.365799 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.365924 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.367296 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.367663 140302422521728 create_pretraining_data.py:151] tokens: [CLS] a program ##mable [MASK] device comprising : a bits ##tream deco ##lle ##ress ##or adapted to deco ##mp ##ress a compressed configuration bits ##tream comprised of a plurality of code ##words into corresponding data words of an un ##com ##pressed configuration bits ##tream , the [MASK] [MASK] deco ##mp ##ress ##or including : a buffer for receiving the compressed configuration bits ##tream [MASK] the buffer including shifting logic configured to shift the bits ##tream through the buffer such that following a shift of a code ##word out of the buffer , a code ##word to - be - deco ##ded in the buffer has a known position in the buffer ; and a huff ##man deco ##der for deco ##ding the code ##word to - be - deco ##ded in the known position in the buffer , wherein the huff ##man deco ##der includes a [MASK] multiple ##x ##er adapted to select from a set [unused588] more - prevalent data words and a second multiple ##x ##er adapted [MASK] select from a null data word and the output of the first multiple ##x ##er . the program ##mable logic device of claim 1 , wherein the buffer stores the received bits ##tream in a memory arranged from a first bit to a last bit , and wherein the buffer further includes a pointer [MASK] configured to generate a pointer signal indicating a bit position in the memory of a next - to - be - deco ##ded code ##word . [SEP] the program ##mable logic device of claim 1 , wherein the second multiple ##x ##er is further adapted to select from the null data word , the output of the first multiple [MASK] ##er , and the buffer ' s memory . a program ##mable logic device including a bits ##tream deco [MASK] accountants ##or for deco ##mp ##ress ##ing a configuration bits ##tream containing configuration [MASK] encoded there ##in , the bits ##tream deco ##mp ##ress ##or comprising : a huff ##man deco ##der adapted to deco ##de a code ##word within the configuration bits ##tream into a corresponding data word , the huff ##man [MASK] ##der including : a first multiple ##x ##er responsive to certain bits within a code ##word for selecting a [MASK] data word that is racism in the device and is from a set of more - prevalent data words included in the configuration data ; and a second multiple ##x ##er responsive to a flag within the code ##word for selecting a corresponding data word from the data word selected by the first multiple ##x ##er and a data word that is the most - prevalent data word in the configuration data . the program ##mable logic device of claim 1 , [MASK] the second multiple ##x ##er is further responsive to the flag within the code ##word for selecting a corresponding data word from the most - prevalent data word , the data word selected by the first multiple ##x ##er , and a data [MASK] within the code ##word . [SEP]\n",
            "I0811 12:00:06.367920 140302422521728 create_pretraining_data.py:161] input_ids: 101 1037 2565 24088 103 5080 9605 1024 1037 9017 25379 21933 6216 8303 2953 5967 2000 21933 8737 8303 1037 16620 9563 9017 25379 11539 1997 1037 29018 1997 3642 22104 2046 7978 2951 2616 1997 2019 4895 9006 19811 9563 9017 25379 1010 1996 103 103 21933 8737 8303 2953 2164 1024 1037 17698 2005 4909 1996 16620 9563 9017 25379 103 1996 17698 2164 9564 7961 26928 2000 5670 1996 9017 25379 2083 1996 17698 2107 2008 2206 1037 5670 1997 1037 3642 18351 2041 1997 1996 17698 1010 1037 3642 18351 2000 1011 2022 1011 21933 5732 1999 1996 17698 2038 1037 2124 2597 1999 1996 17698 1025 1998 1037 21301 2386 21933 4063 2005 21933 4667 1996 3642 18351 2000 1011 2022 1011 21933 5732 1999 1996 2124 2597 1999 1996 17698 1010 16726 1996 21301 2386 21933 4063 2950 1037 103 3674 2595 2121 5967 2000 7276 2013 1037 2275 593 2062 1011 15157 2951 2616 1998 1037 2117 3674 2595 2121 5967 103 7276 2013 1037 19701 2951 2773 1998 1996 6434 1997 1996 2034 3674 2595 2121 1012 1996 2565 24088 7961 5080 1997 4366 1015 1010 16726 1996 17698 5324 1996 2363 9017 25379 1999 1037 3638 5412 2013 1037 2034 2978 2000 1037 2197 2978 1010 1998 16726 1996 17698 2582 2950 1037 20884 103 26928 2000 9699 1037 20884 4742 8131 1037 2978 2597 1999 1996 3638 1997 1037 2279 1011 2000 1011 2022 1011 21933 5732 3642 18351 1012 102 1996 2565 24088 7961 5080 1997 4366 1015 1010 16726 1996 2117 3674 2595 2121 2003 2582 5967 2000 7276 2013 1996 19701 2951 2773 1010 1996 6434 1997 1996 2034 3674 103 2121 1010 1998 1996 17698 1005 1055 3638 1012 1037 2565 24088 7961 5080 2164 1037 9017 25379 21933 103 29114 2953 2005 21933 8737 8303 2075 1037 9563 9017 25379 4820 9563 103 12359 2045 2378 1010 1996 9017 25379 21933 8737 8303 2953 9605 1024 1037 21301 2386 21933 4063 5967 2000 21933 3207 1037 3642 18351 2306 1996 9563 9017 25379 2046 1037 7978 2951 2773 1010 1996 21301 2386 103 4063 2164 1024 1037 2034 3674 2595 2121 26651 2000 3056 9017 2306 1037 3642 18351 2005 17739 1037 103 2951 2773 2008 2003 14398 1999 1996 5080 1998 2003 2013 1037 2275 1997 2062 1011 15157 2951 2616 2443 1999 1996 9563 2951 1025 1998 1037 2117 3674 2595 2121 26651 2000 1037 5210 2306 1996 3642 18351 2005 17739 1037 7978 2951 2773 2013 1996 2951 2773 3479 2011 1996 2034 3674 2595 2121 1998 1037 2951 2773 2008 2003 1996 2087 1011 15157 2951 2773 1999 1996 9563 2951 1012 1996 2565 24088 7961 5080 1997 4366 1015 1010 103 1996 2117 3674 2595 2121 2003 2582 26651 2000 1996 5210 2306 1996 3642 18351 2005 17739 1037 7978 2951 2773 2013 1996 2087 1011 15157 2951 2773 1010 1996 2951 2773 3479 2011 1996 2034 3674 2595 2121 1010 1998 1037 2951 103 2306 1996 3642 18351 1012 102\n",
            "I0811 12:00:06.368139 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.368350 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.368435 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 4 12 46 47 63 146 156 166 169 224 284 304 305 318 358 378 383 461 479 505\n",
            "I0811 12:00:06.368517 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 7961 8737 9017 25379 1010 2034 1997 2595 2000 7961 2595 8737 8303 2951 21933 7978 8250 16726 1037 2773\n",
            "I0811 12:00:06.368607 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.368701 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.369598 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.369935 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the communications system of claim 23 , further vamp a controller coupled to the circuit to operate the circuit to wireless ##ly transmit the communications signal in accordance with an orthogonal frequency division multiple ##xing protocol . the communications system of claim 23 , wherein the communications system comprises a selected one from the group consisting of a cellular phone , a personal digital assistant , a hand held gaming device , a video display device , and a video camera . [SEP] the apparatus [MASK] : a first winding [MASK] ; a [MASK] winding roll located a distance from the first winding roll to define a technological ni ##p there ##bet ##wee ##n ; a [MASK] support plate on which the core is received [MASK] moved toward the winding ni ##p ; and a web separation bar mo ##vable toward the web to separate the web ; wherein the web separation bar comprises a base and a tip , and wherein the tip contacts the web on both sides of a per [MASK] ##ation and breaks the web along the per ##for ##ation . the apparatus of claim 60 , wherein the tip includes a first portion and a second [MASK] such that flags tip stretches the web between the first and second portions until the [MASK] separates along the per ##for ##ation . the apparatus of claim 61 , [MASK] the tip is composed of a res ##ili [MASK] material such that stretching the web between the first and second portions spreads the first and second portions apart with respect to each other . the apparatus of claim 61 , wherein the tip is substantially y - shaped . the apparatus of claim 60 , wherein the per ##for ##ation is substantially centered between the first and second portions of the tip when the tip contacts the web . the apparatus of claim 60 , wherein the web separation bar is rot ##atable into and out of pressing relationship with the web . the apparatus of claim 60 , wherein the web separation [MASK] is accelerated [MASK] a velocity substantially equal to the velocity of the web . a method of winding a per ##for ##ated web onto a core in a re ##wind ##er , the [MASK] ##wind ##er winding a web of material adjacent at least one of a first winding roll and a second winding roll , the first and second winding rolls defining a ni ##p in the re ##wind ##er , the method comprising : passing the web over a surface of the first winding roll ; moving a [MASK] onto at least one core support plate and toward the ni ##p ; moving a web separation bar [MASK] the web , pressing [MASK] web between the web separation bar and a surface on the opposite side of the web ; contact ##ing the web on both sides of a per ##for ##ation with the separation bar , separating the web into a leading edge and a trailing edge ; moving the web [SEP]\n",
            "I0811 12:00:06.370170 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 4806 2291 1997 4366 2603 1010 2582 20279 1037 11486 11211 2000 1996 4984 2000 5452 1996 4984 2000 9949 2135 19818 1996 4806 4742 1999 10388 2007 2019 28721 6075 2407 3674 19612 8778 1012 1996 4806 2291 1997 4366 2603 1010 16726 1996 4806 2291 8681 1037 3479 2028 2013 1996 2177 5398 1997 1037 12562 3042 1010 1037 3167 3617 3353 1010 1037 2192 2218 10355 5080 1010 1037 2678 4653 5080 1010 1998 1037 2678 4950 1012 102 1996 14709 103 1024 1037 2034 12788 103 1025 1037 103 12788 4897 2284 1037 3292 2013 1996 2034 12788 4897 2000 9375 1037 10660 9152 2361 2045 20915 28394 2078 1025 1037 103 2490 5127 2006 2029 1996 4563 2003 2363 103 2333 2646 1996 12788 9152 2361 1025 1998 1037 4773 8745 3347 9587 12423 2646 1996 4773 2000 3584 1996 4773 1025 16726 1996 4773 8745 3347 8681 1037 2918 1998 1037 5955 1010 1998 16726 1996 5955 10402 1996 4773 2006 2119 3903 1997 1037 2566 103 3370 1998 7807 1996 4773 2247 1996 2566 29278 3370 1012 1996 14709 1997 4366 3438 1010 16726 1996 5955 2950 1037 2034 4664 1998 1037 2117 103 2107 2008 9245 5955 14082 1996 4773 2090 1996 2034 1998 2117 8810 2127 1996 103 18600 2247 1996 2566 29278 3370 1012 1996 14709 1997 4366 6079 1010 103 1996 5955 2003 3605 1997 1037 24501 18622 103 3430 2107 2008 10917 1996 4773 2090 1996 2034 1998 2117 8810 20861 1996 2034 1998 2117 8810 4237 2007 4847 2000 2169 2060 1012 1996 14709 1997 4366 6079 1010 16726 1996 5955 2003 12381 1061 1011 5044 1012 1996 14709 1997 4366 3438 1010 16726 1996 2566 29278 3370 2003 12381 8857 2090 1996 2034 1998 2117 8810 1997 1996 5955 2043 1996 5955 10402 1996 4773 1012 1996 14709 1997 4366 3438 1010 16726 1996 4773 8745 3347 2003 18672 27892 2046 1998 2041 1997 7827 3276 2007 1996 4773 1012 1996 14709 1997 4366 3438 1010 16726 1996 4773 8745 103 2003 14613 103 1037 10146 12381 5020 2000 1996 10146 1997 1996 4773 1012 1037 4118 1997 12788 1037 2566 29278 4383 4773 3031 1037 4563 1999 1037 2128 11101 2121 1010 1996 103 11101 2121 12788 1037 4773 1997 3430 5516 2012 2560 2028 1997 1037 2034 12788 4897 1998 1037 2117 12788 4897 1010 1996 2034 1998 2117 12788 9372 12854 1037 9152 2361 1999 1996 2128 11101 2121 1010 1996 4118 9605 1024 4458 1996 4773 2058 1037 3302 1997 1996 2034 12788 4897 1025 3048 1037 103 3031 2012 2560 2028 4563 2490 5127 1998 2646 1996 9152 2361 1025 3048 1037 4773 8745 3347 103 1996 4773 1010 7827 103 4773 2090 1996 4773 8745 3347 1998 1037 3302 2006 1996 4500 2217 1997 1996 4773 1025 3967 2075 1996 4773 2006 2119 3903 1997 1037 2566 29278 3370 2007 1996 8745 3347 1010 14443 1996 4773 2046 1037 2877 3341 1998 1037 12542 3341 1025 3048 1996 4773 102\n",
            "I0811 12:00:06.370382 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.469543 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.469848 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 9 86 91 94 108 117 126 136 174 202 205 218 232 241 346 349 380 437 456 461\n",
            "I0811 12:00:06.469960 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 9605 9605 4897 2117 12788 4563 1998 4773 29278 4664 1996 4773 16726 4765 3347 2000 2128 4563 2646 1996\n",
            "I0811 12:00:06.470092 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.470182 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.471538 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.472040 140302422521728 create_pretraining_data.py:151] tokens: [CLS] chip as [MASK] master chip and other chips as slave chips ; b ) training a chip by a method comprising wholly i ) enabling a driver on a first chip and di ##sa ##bling a driver [MASK] a second chip at a beginning of a training period ; ii ) sending a training pulse from the first chip to the second chip to arrive at the second chip after a certain delay ; iii ) turning off the driver [MASK] the first chip ; iv [MASK] detecting the pulse on the second chip , and turning on the driver on [MASK] [MASK] chip to [MASK] the pulse back to [MASK] first chip ; v ) turning off the driver on the second chip after the pulse has been sent ; c ) sending out a training signal from the master chip to the slave chips to determine late ##ncy from the master chip to a slave chip ; d ) sending out a sync [MASK] ##oni ##zation signal to sync ##hr ##oni ##ze a & # x ##20 ##1 ##c ; time zero & # x ##20 ##1 ##d ; of the master and slave chips and e ) after the pulse is sent , and the driver on the first chip is turned off , a drain device at a driving end keeps a line from floating by adding a delay before di ##sa ##bling the driver on the first chip , the added delay is chosen to be larger than any one [SEP] least one of ( a ) rounded and ( b ) arranged as an [MASK] [MASK] fl ##ange . the damp ##ing element according to claim 11 , wherein the at [MASK] one elevation on the side of the second portion facing the fuel injection valve is arranged at least one of ( a ) on an end of the second portion facing away from the first portion and ( b ) approximately halfway along a length of the [MASK] portion facing away from the first portion . a damp ##ing element for a fuel injection valve insert ##able into a receiving con ##du ##it of a cylinder head of an internal combustion engine , the damp ##ing element arranged between a valve housing of the fuel injection valve and a wall of the receiving con ##du ##it of the cylinder head , wherein : the damp ##ing element includes a first portion for bracing against [MASK] shoulder of the receiving con ##du ##it of the cylinder head and a second portion for bracing of the fuel injection valve ; the damp ##ing element is arranged in [MASK] plate - shaped fashion ; the first portion is angled with respect to the second portion and extends from the second portion radial ##ly inward with respect to a valve screaming [MASK] and the second portion is arranged conical ##ly and includes at least one elevation adapted to form a support for the fuel injection valve [MASK] on an upper side facing the fuel injection valve [SEP]\n",
            "I0811 12:00:06.473903 140302422521728 create_pretraining_data.py:161] input_ids: 101 9090 2004 103 3040 9090 1998 2060 11772 2004 6658 11772 1025 1038 1007 2731 1037 9090 2011 1037 4118 9605 12590 1045 1007 12067 1037 4062 2006 1037 2034 9090 1998 4487 3736 9709 1037 4062 103 1037 2117 9090 2012 1037 2927 1997 1037 2731 2558 1025 2462 1007 6016 1037 2731 8187 2013 1996 2034 9090 2000 1996 2117 9090 2000 7180 2012 1996 2117 9090 2044 1037 3056 8536 1025 3523 1007 3810 2125 1996 4062 103 1996 2034 9090 1025 4921 103 25952 1996 8187 2006 1996 2117 9090 1010 1998 3810 2006 1996 4062 2006 103 103 9090 2000 103 1996 8187 2067 2000 103 2034 9090 1025 1058 1007 3810 2125 1996 4062 2006 1996 2117 9090 2044 1996 8187 2038 2042 2741 1025 1039 1007 6016 2041 1037 2731 4742 2013 1996 3040 9090 2000 1996 6658 11772 2000 5646 2397 9407 2013 1996 3040 9090 2000 1037 6658 9090 1025 1040 1007 6016 2041 1037 26351 103 10698 9276 4742 2000 26351 8093 10698 4371 1037 1004 1001 1060 11387 2487 2278 1025 2051 5717 1004 1001 1060 11387 2487 2094 1025 1997 1996 3040 1998 6658 11772 1998 1041 1007 2044 1996 8187 2003 2741 1010 1998 1996 4062 2006 1996 2034 9090 2003 2357 2125 1010 1037 12475 5080 2012 1037 4439 2203 7906 1037 2240 2013 8274 2011 5815 1037 8536 2077 4487 3736 9709 1996 4062 2006 1996 2034 9090 1010 1996 2794 8536 2003 4217 2000 2022 3469 2084 2151 2028 102 2560 2028 1997 1006 1037 1007 8352 1998 1006 1038 1007 5412 2004 2019 103 103 13109 22043 1012 1996 10620 2075 5783 2429 2000 4366 2340 1010 16726 1996 2012 103 2028 6678 2006 1996 2217 1997 1996 2117 4664 5307 1996 4762 13341 10764 2003 5412 2012 2560 2028 1997 1006 1037 1007 2006 2019 2203 1997 1996 2117 4664 5307 2185 2013 1996 2034 4664 1998 1006 1038 1007 3155 8576 2247 1037 3091 1997 1996 103 4664 5307 2185 2013 1996 2034 4664 1012 1037 10620 2075 5783 2005 1037 4762 13341 10764 19274 3085 2046 1037 4909 9530 8566 4183 1997 1037 7956 2132 1997 2019 4722 16513 3194 1010 1996 10620 2075 5783 5412 2090 1037 10764 3847 1997 1996 4762 13341 10764 1998 1037 2813 1997 1996 4909 9530 8566 4183 1997 1996 7956 2132 1010 16726 1024 1996 10620 2075 5783 2950 1037 2034 4664 2005 25919 2114 103 3244 1997 1996 4909 9530 8566 4183 1997 1996 7956 2132 1998 1037 2117 4664 2005 25919 1997 1996 4762 13341 10764 1025 1996 10620 2075 5783 2003 5412 1999 103 5127 1011 5044 4827 1025 1996 2034 4664 2003 18756 2007 4847 2000 1996 2117 4664 1998 8908 2013 1996 2117 4664 15255 2135 20546 2007 4847 2000 1037 10764 7491 103 1998 1996 2117 4664 2003 5412 24750 2135 1998 2950 2012 2560 2028 6678 5967 2000 2433 1037 2490 2005 1996 4762 13341 10764 103 2006 2019 3356 2217 5307 1996 4762 13341 10764 102\n",
            "I0811 12:00:06.474323 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.475173 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.475340 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 3 22 38 81 87 102 103 106 111 166 271 272 288 336 366 413 444 475 476 501\n",
            "I0811 12:00:06.475466 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1037 1024 2006 2006 1007 1996 2117 2709 1996 8093 5754 7934 2560 2117 1997 1037 1037 8123 1025 1010\n",
            "I0811 12:00:06.475592 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.475713 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.476993 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.477396 140302422521728 create_pretraining_data.py:151] tokens: [CLS] of a t ##lb purge is indicative that a change in virtual - memory - to - physical [MASK] memory mapping has occurred . a method for controlling virtual memory address translation during data movement [MASK] enabled in a hardware environment , [MASK] the [MASK] of : monitoring , as a hardware operation , for an occurrence of a translation look ##asi ##de buffer ( t ##lb ) purge [MASK] setup and execution of a data movement operation from virtual memory ; upon detection [MASK] a t ##lb purge prior to completion of the data movement operation , ab ##ort ##ing the data movement operation pending rees ##ta ##bl ##ishment of accurate virtual - memory - to - physical - memory mapping ; and [MASK] ##que ##uing status information on whether the data [MASK] operation completed or was ab ##orted . the method of claim 5 , wherein said occurrence of a t ##lb purge is indicative that a change in virtual - memory - to - physical - memory mapping [MASK] occurred . the method of claim 5 further comprising : en ##que ##uing status information including [MASK] of data that was successfully moved prior [MASK] the ab ##ort . hardware for controlling virtual memory address [MASK] during data operations involving physical movement of data , the hardware comprising : means for setting a first flag upon initiation of a data operation ; means for periodically monitoring for translation look ##asi ##de buffer ( t ##lb ) purge ##s ; means for translating virtual address space to physical address space ; means for setting up one or [MASK] input registers on a data move ##r ; means , responsive to said means for translating [MASK] said means for setting up , for clearing the first flag [MASK] a t ##lb purge has been detected ; and means for ab ##ort ##ing the data operation and then en ##que ##uing a [SEP] the hardware of claim 8 , in which the first operation completion status identifies data that was successfully moved prior to the ab ##ort . the hardware of claim 8 , in which the data operation is a data copying operation . the hardware of claim 8 , in which the means for clearing the first flag is enabled if a t ##lb purge has been detected before physical data . the hardware of claim 8 , wherein an occurrence of a translation look ##asi ##de buffer ( t ##lb ) purge during setup and execution of a data movement operation from virtual memory is indicative [MASK] a change in virtual [MASK] memory - to - physical - memory mapping has occurred [MASK] the hardware of claim 8 further comprising : means , responsive to said means for translating and said means for [MASK] up , for clearing the first flag and setting a second flag if a t ##lb purge has not been detected ; means for examining the second flag ; and means for commencing physical movement of data if the second flag is set . [SEP]\n",
            "I0811 12:00:06.477799 140302422521728 create_pretraining_data.py:161] input_ids: 101 1997 1037 1056 20850 24694 2003 24668 2008 1037 2689 1999 7484 1011 3638 1011 2000 1011 3558 103 3638 12375 2038 4158 1012 1037 4118 2005 9756 7484 3638 4769 5449 2076 2951 2929 103 9124 1999 1037 8051 4044 1010 103 1996 103 1997 1024 8822 1010 2004 1037 8051 3169 1010 2005 2019 14404 1997 1037 5449 2298 21369 3207 17698 1006 1056 20850 1007 24694 103 16437 1998 7781 1997 1037 2951 2929 3169 2013 7484 3638 1025 2588 10788 103 1037 1056 20850 24694 3188 2000 6503 1997 1996 2951 2929 3169 1010 11113 11589 2075 1996 2951 2929 3169 14223 22131 2696 16558 21808 1997 8321 7484 1011 3638 1011 2000 1011 3558 1011 3638 12375 1025 1998 103 4226 25165 3570 2592 2006 3251 1996 2951 103 3169 2949 2030 2001 11113 15613 1012 1996 4118 1997 4366 1019 1010 16726 2056 14404 1997 1037 1056 20850 24694 2003 24668 2008 1037 2689 1999 7484 1011 3638 1011 2000 1011 3558 1011 3638 12375 103 4158 1012 1996 4118 1997 4366 1019 2582 9605 1024 4372 4226 25165 3570 2592 2164 103 1997 2951 2008 2001 5147 2333 3188 103 1996 11113 11589 1012 8051 2005 9756 7484 3638 4769 103 2076 2951 3136 5994 3558 2929 1997 2951 1010 1996 8051 9605 1024 2965 2005 4292 1037 2034 5210 2588 17890 1997 1037 2951 3169 1025 2965 2005 18043 8822 2005 5449 2298 21369 3207 17698 1006 1056 20850 1007 24694 2015 1025 2965 2005 22969 7484 4769 2686 2000 3558 4769 2686 1025 2965 2005 4292 2039 2028 2030 103 7953 18687 2006 1037 2951 2693 2099 1025 2965 1010 26651 2000 2056 2965 2005 22969 103 2056 2965 2005 4292 2039 1010 2005 8430 1996 2034 5210 103 1037 1056 20850 24694 2038 2042 11156 1025 1998 2965 2005 11113 11589 2075 1996 2951 3169 1998 2059 4372 4226 25165 1037 102 1996 8051 1997 4366 1022 1010 1999 2029 1996 2034 3169 6503 3570 14847 2951 2008 2001 5147 2333 3188 2000 1996 11113 11589 1012 1996 8051 1997 4366 1022 1010 1999 2029 1996 2951 3169 2003 1037 2951 24731 3169 1012 1996 8051 1997 4366 1022 1010 1999 2029 1996 2965 2005 8430 1996 2034 5210 2003 9124 2065 1037 1056 20850 24694 2038 2042 11156 2077 3558 2951 1012 1996 8051 1997 4366 1022 1010 16726 2019 14404 1997 1037 5449 2298 21369 3207 17698 1006 1056 20850 1007 24694 2076 16437 1998 7781 1997 1037 2951 2929 3169 2013 7484 3638 2003 24668 103 1037 2689 1999 7484 103 3638 1011 2000 1011 3558 1011 3638 12375 2038 4158 103 1996 8051 1997 4366 1022 2582 9605 1024 2965 1010 26651 2000 2056 2965 2005 22969 1998 2056 2965 2005 103 2039 1010 2005 8430 1996 2034 5210 1998 4292 1037 2117 5210 2065 1037 1056 20850 24694 2038 2025 2042 11156 1025 2965 2005 12843 1996 2117 5210 1025 1998 2965 2005 25819 3558 2929 1997 2951 2065 1996 2117 5210 2003 2275 1012 102\n",
            "I0811 12:00:06.478120 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.480363 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.480514 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 19 36 43 45 70 85 125 134 172 189 197 208 269 286 298 429 434 445 466 500\n",
            "I0811 12:00:06.480632 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1011 3136 9605 4084 2076 1997 4372 2929 2038 8720 2000 5449 2062 1998 2065 2008 1011 1012 4292 3558\n",
            "I0811 12:00:06.480766 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.480867 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.482000 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.482357 140302422521728 create_pretraining_data.py:151] tokens: [CLS] a method according to claim 10 , wherein the static random access memory [MASK] is a sub ##th ##resh ##old circuit , and has a single read / write port . [SEP] rated variable increases . the method as claimed in claim 2 , wherein the [MASK] power in the control range is increased continuously as the disc ##re ##pan ##cy between the electrical variable and the rated variable decreases . the method as claimed in claim 2 , wherein the power transmission is interrupted in the event of disc ##re ##pan ##cies of the electrical variable which exceed the control range . the method as chiefs in claim 2 , wherein a second control range between a third and a fourth threshold value for the [MASK] variable is defined in which the power transmission is changed on the basis of the electrical variable , wherein the rated variable is between the second control range and the first control range . the method as claimed in claim 6 , wherein the transmitted power in [MASK] second control range is increased continuously as the disc ##re ##pan ##cy between the electrical variable and the rated variable increases . the method as claimed in claim 6 , wherein the transmitted power in the second control range is decreased [MASK] as the disc ##re ##pan ##cy between the electrical variable and the rated variable decreases . the method as [MASK] in claim [MASK] , wherein the threshold values [MASK] the electrical variable are fixed on the basis of the rated variable . the method as claimed in claim 1 , wherein the rated variable is determined on the basis of the detected electrical variable . the method as claimed in claim 2 , wherein the power is transmitted from the energy supply grid to the energy consumer , and wherein the first control range is defined below the rated variable . the method as claimed in claim 2 , wherein the power is transmitted from the energy generation unit to [MASK] energy [MASK] grid , [MASK] wherein the first control range defined above the rated variable . the method as claimed in claim 1 , wherein the control of the power transmission is switched off on the basis of an operating condition of the power transmission . an apparatus for transmitting electrical power between an energy supply grid and an energy consumer or energy generation unit which is coupled to the energy supply grid , said apparatus comprising : a detection unit for detecting at least one electrical variable of the energy supply grid , an evaluation unit for determining a disc ##re ##pan ##cy between the electrical variable and a rated variable of the energy supply grid assigned to the electrical variable , and a control unit [MASK] which is designed to control the power transmission on the basis of the disc ##re ##pan ##cy , wherein the electrical variable is selected from the group consisting of a line [MASK] and a main ##s voltage of the energy supply grid [MASK] the [SEP]\n",
            "I0811 12:00:06.482634 140302422521728 create_pretraining_data.py:161] input_ids: 101 1037 4118 2429 2000 4366 2184 1010 16726 1996 10763 6721 3229 3638 103 2003 1037 4942 2705 21898 11614 4984 1010 1998 2038 1037 2309 3191 1013 4339 3417 1012 102 6758 8023 7457 1012 1996 4118 2004 3555 1999 4366 1016 1010 16726 1996 103 2373 1999 1996 2491 2846 2003 3445 10843 2004 1996 5860 2890 9739 5666 2090 1996 5992 8023 1998 1996 6758 8023 17913 1012 1996 4118 2004 3555 1999 4366 1016 1010 16726 1996 2373 6726 2003 7153 1999 1996 2724 1997 5860 2890 9739 9243 1997 1996 5992 8023 2029 13467 1996 2491 2846 1012 1996 4118 2004 9058 1999 4366 1016 1010 16726 1037 2117 2491 2846 2090 1037 2353 1998 1037 2959 11207 3643 2005 1996 103 8023 2003 4225 1999 2029 1996 2373 6726 2003 2904 2006 1996 3978 1997 1996 5992 8023 1010 16726 1996 6758 8023 2003 2090 1996 2117 2491 2846 1998 1996 2034 2491 2846 1012 1996 4118 2004 3555 1999 4366 1020 1010 16726 1996 11860 2373 1999 103 2117 2491 2846 2003 3445 10843 2004 1996 5860 2890 9739 5666 2090 1996 5992 8023 1998 1996 6758 8023 7457 1012 1996 4118 2004 3555 1999 4366 1020 1010 16726 1996 11860 2373 1999 1996 2117 2491 2846 2003 10548 103 2004 1996 5860 2890 9739 5666 2090 1996 5992 8023 1998 1996 6758 8023 17913 1012 1996 4118 2004 103 1999 4366 103 1010 16726 1996 11207 5300 103 1996 5992 8023 2024 4964 2006 1996 3978 1997 1996 6758 8023 1012 1996 4118 2004 3555 1999 4366 1015 1010 16726 1996 6758 8023 2003 4340 2006 1996 3978 1997 1996 11156 5992 8023 1012 1996 4118 2004 3555 1999 4366 1016 1010 16726 1996 2373 2003 11860 2013 1996 2943 4425 8370 2000 1996 2943 7325 1010 1998 16726 1996 2034 2491 2846 2003 4225 2917 1996 6758 8023 1012 1996 4118 2004 3555 1999 4366 1016 1010 16726 1996 2373 2003 11860 2013 1996 2943 4245 3131 2000 103 2943 103 8370 1010 103 16726 1996 2034 2491 2846 4225 2682 1996 6758 8023 1012 1996 4118 2004 3555 1999 4366 1015 1010 16726 1996 2491 1997 1996 2373 6726 2003 7237 2125 2006 1996 3978 1997 2019 4082 4650 1997 1996 2373 6726 1012 2019 14709 2005 23820 5992 2373 2090 2019 2943 4425 8370 1998 2019 2943 7325 2030 2943 4245 3131 2029 2003 11211 2000 1996 2943 4425 8370 1010 2056 14709 9605 1024 1037 10788 3131 2005 25952 2012 2560 2028 5992 8023 1997 1996 2943 4425 8370 1010 2019 9312 3131 2005 12515 1037 5860 2890 9739 5666 2090 1996 5992 8023 1998 1037 6758 8023 1997 1996 2943 4425 8370 4137 2000 1996 5992 8023 1010 1998 1037 2491 3131 103 2029 2003 2881 2000 2491 1996 2373 6726 2006 1996 3978 1997 1996 5860 2890 9739 5666 1010 16726 1996 5992 8023 2003 3479 2013 1996 2177 5398 1997 1037 2240 103 1998 1037 2364 2015 10004 1997 1996 2943 4425 8370 103 1996 102\n",
            "I0811 12:00:06.482893 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.483190 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.483323 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 10 14 47 107 127 145 175 217 229 237 240 246 269 281 338 340 343 466 498 509\n",
            "I0811 12:00:06.483429 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 10763 3526 11860 3555 5992 1010 1996 10843 1996 3555 1016 2005 1996 8023 1996 4425 1998 1010 6075 1998\n",
            "I0811 12:00:06.483541 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.483637 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.484513 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.484869 140302422521728 create_pretraining_data.py:151] tokens: [CLS] score lines between the first cover and the second cover to facilitate detachment of the [MASK] patch cord from the second patch cord . the patch [MASK] assembly of claim 24 , wherein the first and second covers are configured and dimension ##ed to permit ##laus [MASK] the first plug and second plug . the patch cord assembly of claim 26 , wherein the first and second covers are configured and dimension [MASK] to be wrapped around the first and second elongated cords for det ##ach ##ably securing the first and second patch cords . the patch cord assembly of claim 1 , wherein the first couple ##r element is configured as a first ring and a second ring connected to a base . the patch cord assembly of claim 28 , wherein the first and second rings are configured as substantially semi [MASK] ##rc ##ular rings . the patch cord assembly of claim 29 , wherein the first and second rings are configured and dimension ##ed to rotate and secure a first plug and a [MASK] plug between the first and second rings and the base . the patch cord assembly of claim 1 , wherein the first couple ##r element is [MASK] as a first ring connected to a base . the patch cord assembly of claim 31 , wherein the first ring is configured as a substantially circular ring . the patch cord [MASK] of claim 32 , wherein the first ring is configured and dimension ##ed to rotate and secure a [SEP] further comprising a layer of pressure - sensitive ad ##hesive in between said adjacent layers wherein said rf ##id device is 夏 , said pressure - sensitive ad ##hesive layer having sufficient thickness ##stock ensure continuous and intimate contact with both said adjacent layers [MASK] the rf ##id device , to thereby ensure a complete seal between those layers around the rf ##id device . the pal ##let structure claim 27 , said pressure - [MASK] ad ##hesive layer having a thickness approximately equal to that of said rf ##id device . the pal ##let structure of claim 15 , said pl ##aca ##rd further including [MASK] upper intermediate layer and a lower intermediate [MASK] in between said top and bottom layers , and a plurality of ad ##hesive layers between adjacent ones of said top layer , upper intermediate layer , lower intermediate layer and bottom layer . a wide ##band antenna comprising : a first plan ##ar radiating element and second plan ##ar radiating [MASK] that include at least two sides , wherein at least one of the first and second radiating [MASK] includes a strip - shaped element ; a first side of the first radiating element and a second side of the second radiating element are so disposed as to be parallel to each other , face each other and be shifted in a [MASK] direction ; and the strip - shaped element is so disposed as to be connected to any side other than the first and second sides [SEP]\n",
            "I0811 12:00:06.485117 140302422521728 create_pretraining_data.py:161] input_ids: 101 3556 3210 2090 1996 2034 3104 1998 1996 2117 3104 2000 10956 11009 1997 1996 103 8983 11601 2013 1996 2117 8983 11601 1012 1996 8983 103 3320 1997 4366 2484 1010 16726 1996 2034 1998 2117 4472 2024 26928 1998 9812 2098 2000 9146 28128 103 1996 2034 13354 1998 2117 13354 1012 1996 8983 11601 3320 1997 4366 2656 1010 16726 1996 2034 1998 2117 4472 2024 26928 1998 9812 103 2000 2022 5058 2105 1996 2034 1998 2117 17876 24551 2005 20010 6776 8231 12329 1996 2034 1998 2117 8983 24551 1012 1996 8983 11601 3320 1997 4366 1015 1010 16726 1996 2034 3232 2099 5783 2003 26928 2004 1037 2034 3614 1998 1037 2117 3614 4198 2000 1037 2918 1012 1996 8983 11601 3320 1997 4366 2654 1010 16726 1996 2034 1998 2117 7635 2024 26928 2004 12381 4100 103 11890 7934 7635 1012 1996 8983 11601 3320 1997 4366 2756 1010 16726 1996 2034 1998 2117 7635 2024 26928 1998 9812 2098 2000 24357 1998 5851 1037 2034 13354 1998 1037 103 13354 2090 1996 2034 1998 2117 7635 1998 1996 2918 1012 1996 8983 11601 3320 1997 4366 1015 1010 16726 1996 2034 3232 2099 5783 2003 103 2004 1037 2034 3614 4198 2000 1037 2918 1012 1996 8983 11601 3320 1997 4366 2861 1010 16726 1996 2034 3614 2003 26928 2004 1037 12381 8206 3614 1012 1996 8983 11601 103 1997 4366 3590 1010 16726 1996 2034 3614 2003 26928 1998 9812 2098 2000 24357 1998 5851 1037 102 2582 9605 1037 6741 1997 3778 1011 7591 4748 21579 1999 2090 2056 5516 9014 16726 2056 21792 3593 5080 2003 1808 1010 2056 3778 1011 7591 4748 21579 6741 2383 7182 14983 14758 5676 7142 1998 10305 3967 2007 2119 2056 5516 9014 103 1996 21792 3593 5080 1010 2000 8558 5676 1037 3143 7744 2090 2216 9014 2105 1996 21792 3593 5080 1012 1996 14412 7485 3252 4366 2676 1010 2056 3778 1011 103 4748 21579 6741 2383 1037 14983 3155 5020 2000 2008 1997 2056 21792 3593 5080 1012 1996 14412 7485 3252 1997 4366 2321 1010 2056 20228 19629 4103 2582 2164 103 3356 7783 6741 1998 1037 2896 7783 103 1999 2090 2056 2327 1998 3953 9014 1010 1998 1037 29018 1997 4748 21579 9014 2090 5516 3924 1997 2056 2327 6741 1010 3356 7783 6741 1010 2896 7783 6741 1998 3953 6741 1012 1037 2898 12733 13438 9605 1024 1037 2034 2933 2906 23229 5783 1998 2117 2933 2906 23229 103 2008 2421 2012 2560 2048 3903 1010 16726 2012 2560 2028 1997 1996 2034 1998 2117 23229 103 2950 1037 6167 1011 5044 5783 1025 1037 2034 2217 1997 1996 2034 23229 5783 1998 1037 2117 2217 1997 1996 2117 23229 5783 2024 2061 21866 2004 2000 2022 5903 2000 2169 2060 1010 2227 2169 2060 1998 2022 5429 1999 1037 103 3257 1025 1998 1996 6167 1011 5044 5783 2003 2061 21866 2004 2000 2022 4198 2000 2151 2217 2060 2084 1996 2034 1998 2117 3903 102\n",
            "I0811 12:00:06.581063 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.581604 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.581823 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 16 27 35 46 47 73 144 177 204 237 278 290 301 332 358 363 371 423 441 485\n",
            "I0811 12:00:06.581953 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 2034 11601 2034 23851 1997 2098 6895 2117 26928 3320 11157 2000 2105 7591 20228 2019 6741 5783 3787 5903\n",
            "I0811 12:00:06.582092 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.582204 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.583551 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.583910 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the data processing system of claim 29 , said program instructions comprising generating by the at least one computer processor trades for execution from said selected lots ; and ag ##gre ##gating by the at least one computer processor said generated trades for a virtual portfolio from the one or more virtual portfolio ##s . [SEP] virtual portfolio ##s for each of simpson one or more investors . the system of claim 29 , wherein said financial object comprises : at least one unit of interest in at least one of : an asset ; a liability ; a tracking portfolio ; a financial instrument or a security , wherein said financial instrument or said security denotes at least one of a debt , an equity interest , or a hybrid ; a derivatives [MASK] , comprising at least one of : a future , a forward , a put , a call , an option , a commandant , or any other transaction relating to a flu ##ct ##uation of an underlying asset , notwithstanding the prevailing value of the contract , and notwithstanding whether such contract , for purposes [MASK] accounting , is considered an asset [MASK] liability ; a fund ; or an investment entity or account of any kind , comprising an interest in , or rights relating to at least one of : a hedge fund , an exchange traded fund ( et ##f ) , [MASK] fund of funds , a mutual fund , a closed end fund , an investment vehicle , or any other [MASK] ##d or separately managed investments . a computer - [MASK] method of managing [MASK] or more financial objects , the method being executed [MASK] a data processing system , the method comprising : managing a virtual portfolio of the financial objects by at least one computer comprising a collection of the financial objects managed collectively but tracked separately with separately - owned lots [MASK] behalf of a plurality of investors ; providing a computer database ass ##oc ##iating by the at least one computer , a plurality of holdings owned [MASK] each of said plurality of investors as tr ##ada ##ble , regardless of an in ##itia ##tor of purchase of a holding ; providing by the at least one [MASK] , [MASK] [MASK] rules ; receiving by the at least one computer , a requested trade ; selecting by the at least one computer , a holding for trading from said plurality of holdings using said computer database , said holding selection rules , and said requested trade ; determining by the at least one computer , whether a trade using said holding should be def ##erre ##d using said computer database and said holding selection rules ; and generating by the at least one computer , a def ##erre ##d trade in a tax - managed sub - account associated [MASK] one investor of said plurality of investors using said holding if it is determined that said holding should be sold in order [MASK] realize [SEP]\n",
            "I0811 12:00:06.584161 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 2951 6364 2291 1997 4366 2756 1010 2056 2565 8128 9605 11717 2011 1996 2012 2560 2028 3274 13151 14279 2005 7781 2013 2056 3479 7167 1025 1998 12943 17603 16961 2011 1996 2012 2560 2028 3274 13151 2056 7013 14279 2005 1037 7484 11103 2013 1996 2028 2030 2062 7484 11103 2015 1012 102 7484 11103 2015 2005 2169 1997 9304 2028 2030 2062 9387 1012 1996 2291 1997 4366 2756 1010 16726 2056 3361 4874 8681 1024 2012 2560 2028 3131 1997 3037 1999 2012 2560 2028 1997 1024 2019 11412 1025 1037 14000 1025 1037 9651 11103 1025 1037 3361 6602 2030 1037 3036 1010 16726 2056 3361 6602 2030 2056 3036 14796 2012 2560 2028 1997 1037 7016 1010 2019 10067 3037 1010 2030 1037 8893 1025 1037 16942 103 1010 9605 2012 2560 2028 1997 1024 1037 2925 1010 1037 2830 1010 1037 2404 1010 1037 2655 1010 2019 5724 1010 1037 15254 1010 2030 2151 2060 12598 8800 2000 1037 19857 6593 14505 1997 2019 10318 11412 1010 26206 1996 19283 3643 1997 1996 3206 1010 1998 26206 3251 2107 3206 1010 2005 5682 103 9529 1010 2003 2641 2019 11412 103 14000 1025 1037 4636 1025 2030 2019 5211 9178 2030 4070 1997 2151 2785 1010 9605 2019 3037 1999 1010 2030 2916 8800 2000 2012 2560 2028 1997 1024 1037 17834 4636 1010 2019 3863 7007 4636 1006 3802 2546 1007 1010 103 4636 1997 5029 1010 1037 8203 4636 1010 1037 2701 2203 4636 1010 2019 5211 4316 1010 2030 2151 2060 103 2094 2030 10329 3266 10518 1012 1037 3274 1011 103 4118 1997 6605 103 2030 2062 3361 5200 1010 1996 4118 2108 6472 103 1037 2951 6364 2291 1010 1996 4118 9605 1024 6605 1037 7484 11103 1997 1996 3361 5200 2011 2012 2560 2028 3274 9605 1037 3074 1997 1996 3361 5200 3266 13643 2021 12808 10329 2007 10329 1011 3079 7167 103 6852 1997 1037 29018 1997 9387 1025 4346 1037 3274 7809 4632 10085 15370 2011 1996 2012 2560 2028 3274 1010 1037 29018 1997 9583 3079 103 2169 1997 2056 29018 1997 9387 2004 19817 8447 3468 1010 7539 1997 2019 1999 29050 4263 1997 5309 1997 1037 3173 1025 4346 2011 1996 2012 2560 2028 103 1010 103 103 3513 1025 4909 2011 1996 2012 2560 2028 3274 1010 1037 7303 3119 1025 17739 2011 1996 2012 2560 2028 3274 1010 1037 3173 2005 6202 2013 2056 29018 1997 9583 2478 2056 3274 7809 1010 2056 3173 4989 3513 1010 1998 2056 7303 3119 1025 12515 2011 1996 2012 2560 2028 3274 1010 3251 1037 3119 2478 2056 3173 2323 2022 13366 28849 2094 2478 2056 3274 7809 1998 2056 3173 4989 3513 1025 1998 11717 2011 1996 2012 2560 2028 3274 1010 1037 13366 28849 2094 3119 1999 1037 4171 1011 3266 4942 1011 4070 3378 103 2028 14316 1997 2056 29018 1997 9387 2478 2056 3173 2065 2009 2003 4340 2008 2056 3173 2323 2022 2853 1999 2344 103 5382 102\n",
            "I0811 12:00:06.584387 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.584597 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.584699 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 63 135 141 159 192 199 242 263 273 277 287 327 354 384 386 387 403 413 486 509\n",
            "I0811 12:00:06.584784 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1996 3206 1997 19948 1997 2030 1037 19107 7528 2028 2006 2006 2011 3274 3173 4989 2011 6202 2007 2000\n",
            "I0811 12:00:06.584880 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.584954 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.585839 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.586159 140302422521728 create_pretraining_data.py:151] tokens: [CLS] [MASK] parallel [MASK] acc ##um ##ulator of claim 1 where the number of partition ##s is two . the parallel digital acc ##um ##ulator of claim 2 where the [MASK] partition ##s have the [MASK] number of bits . the parallel digital acc ##um ##ulator [MASK] claim 2 where the two partition ##s have a different number of bits . the parallel digital acc ##um ##ulator of claim 2 where the most significant partition further comprises a delay element between the parallel input bits and the input to the add ##er , and the least significant partition further comprises a delay element between the output of the register and the parallel output bits . the parallel digital acc ##um [MASK] of claim 2 where the most significant partition further comprises a delay element between the parallel input bits and the input to the add ##er , and the output bits of the least significant partition are not used in further computation . the parallel digital acc ##um ##ulator of claim 1 where the number of partition ##s is more than two . the parallel digital acc ##um ##ulator of claim 1 where each partition has [MASK] bit . the parallel [MASK] acc ##um ##ulator of claim 1 where each partition has a plurality of bits . the parallel digital acc ##um ##ulator of claim 1 where each partition except the least significant partition further comprises a delay element between [MASK] input bits and [MASK] add ##er . [SEP] the control panel of claim 1 , wherein the cap ##ac ##itive position sensing element comprises [MASK] continuous resist ##ive sensing path and wherein the position sensing circuit is opera ##ble to determine a cap ##ac ##itan ##ce between the resist ##ive sensing path and a system ground potential at a plurality of [MASK] along the resist ##ive sensing path . the control panel of claim 6 , wherein the switch comprises a [MASK] ##or ##mable dia ##ph ##rag ##m having a resist ##ivity substantially the same pablo that of the resist ##ive sensing path . the control panel of claim 1 , wherein the position sensing element is opera ##ble to determine the position of an object in a sensing direction and wherein the mechanical switch has a greater extent in a direction [MASK] to the sensing direction than in a direction along the sensing direction . the control panel of claim 1 , further comprising a surface panel over ##lay ##ing the position sensing element , wherein the surface panel has a wall defining an opening through which the mechanical switch passes . the control panel of [MASK] 9 , further comprising a protective ##typic membrane over ##lay ##ing the surface panel . the control panel of claim 1 , wherein the mechanical switch comprises a [MASK] ##or ##mable dia ##ph ##rag ##m over ##lay ##ing the switched [MASK] , and wherein the switch is opera ##ble to be changed between the open state and the closed state by pushing on the def ##or ##mable dia ##ph ##rag ##m . [SEP]\n",
            "I0811 12:00:06.586400 140302422521728 create_pretraining_data.py:161] input_ids: 101 103 5903 103 16222 2819 20350 1997 4366 1015 2073 1996 2193 1997 13571 2015 2003 2048 1012 1996 5903 3617 16222 2819 20350 1997 4366 1016 2073 1996 103 13571 2015 2031 1996 103 2193 1997 9017 1012 1996 5903 3617 16222 2819 20350 103 4366 1016 2073 1996 2048 13571 2015 2031 1037 2367 2193 1997 9017 1012 1996 5903 3617 16222 2819 20350 1997 4366 1016 2073 1996 2087 3278 13571 2582 8681 1037 8536 5783 2090 1996 5903 7953 9017 1998 1996 7953 2000 1996 5587 2121 1010 1998 1996 2560 3278 13571 2582 8681 1037 8536 5783 2090 1996 6434 1997 1996 4236 1998 1996 5903 6434 9017 1012 1996 5903 3617 16222 2819 103 1997 4366 1016 2073 1996 2087 3278 13571 2582 8681 1037 8536 5783 2090 1996 5903 7953 9017 1998 1996 7953 2000 1996 5587 2121 1010 1998 1996 6434 9017 1997 1996 2560 3278 13571 2024 2025 2109 1999 2582 22334 1012 1996 5903 3617 16222 2819 20350 1997 4366 1015 2073 1996 2193 1997 13571 2015 2003 2062 2084 2048 1012 1996 5903 3617 16222 2819 20350 1997 4366 1015 2073 2169 13571 2038 103 2978 1012 1996 5903 103 16222 2819 20350 1997 4366 1015 2073 2169 13571 2038 1037 29018 1997 9017 1012 1996 5903 3617 16222 2819 20350 1997 4366 1015 2073 2169 13571 3272 1996 2560 3278 13571 2582 8681 1037 8536 5783 2090 103 7953 9017 1998 103 5587 2121 1012 102 1996 2491 5997 1997 4366 1015 1010 16726 1996 6178 6305 13043 2597 13851 5783 8681 103 7142 9507 3512 13851 4130 1998 16726 1996 2597 13851 4984 2003 3850 3468 2000 5646 1037 6178 6305 25451 3401 2090 1996 9507 3512 13851 4130 1998 1037 2291 2598 4022 2012 1037 29018 1997 103 2247 1996 9507 3512 13851 4130 1012 1996 2491 5997 1997 4366 1020 1010 16726 1996 6942 8681 1037 103 2953 24088 22939 8458 29181 2213 2383 1037 9507 7730 12381 1996 2168 11623 2008 1997 1996 9507 3512 13851 4130 1012 1996 2491 5997 1997 4366 1015 1010 16726 1996 2597 13851 5783 2003 3850 3468 2000 5646 1996 2597 1997 2019 4874 1999 1037 13851 3257 1998 16726 1996 6228 6942 2038 1037 3618 6698 1999 1037 3257 103 2000 1996 13851 3257 2084 1999 1037 3257 2247 1996 13851 3257 1012 1996 2491 5997 1997 4366 1015 1010 2582 9605 1037 3302 5997 2058 8485 2075 1996 2597 13851 5783 1010 16726 1996 3302 5997 2038 1037 2813 12854 2019 3098 2083 2029 1996 6228 6942 5235 1012 1996 2491 5997 1997 103 1023 1010 2582 9605 1037 9474 23186 10804 2058 8485 2075 1996 3302 5997 1012 1996 2491 5997 1997 4366 1015 1010 16726 1996 6228 6942 8681 1037 103 2953 24088 22939 8458 29181 2213 2058 8485 2075 1996 7237 103 1010 1998 16726 1996 6942 2003 3850 3468 2000 2022 2904 2090 1996 2330 2110 1998 1996 2701 2110 2011 6183 2006 1996 13366 2953 24088 22939 8458 29181 2213 1012 102\n",
            "I0811 12:00:06.586615 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.685178 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.685506 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 1 3 30 35 46 47 120 196 201 240 244 265 302 322 336 383 438 445 467 479\n",
            "I0811 12:00:06.685641 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 1996 3617 2048 2168 1997 4366 20350 2028 3617 1996 1996 1037 5269 13366 2004 28721 4366 12379 13366 3967\n",
            "I0811 12:00:06.685799 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.685899 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.687228 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.687727 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the computer system of claim 14 , wherein the at least one controller [MASK] ##rem ##ents the reference count for the at least one data block in response to creating the branch [MASK] . [SEP] network ; ( c ) when immediately after the initial order is contracted , the computer system automatically , without an intervention by the user , generating and placing both a new sell order and a new purchase order through the data communication network according to the automatic trade condition , the sell order being at a price higher than the contracted price [MASK] the initial order and the purchase order being at a price lower than the contracted price for [MASK] initial order ; ( [MASK] ) [MASK] immediately after one of the newly placed sell and purchase orders is contracted , [MASK] computer system automatically , without an intervention by the user , generating and placing a new purchase order and a new sell order for trade according to the automatic trade condition , the sell ##ening being [MASK] a price higher than the [MASK] contracted price and the purchase [MASK] being at a price lower than the previously contracted price ; and ( e [MASK] the computer system repeating the process ( d ) . the method as defined in claim 8 , wherein the automatic trade condition generates selling and purchase order prices increased or decreased by a fixed amount from the previously generated orders . the method as defined in claim 8 , wherein the automatic trade condition generates selling and purchase order prices increased or decreased by a fixed rate from the previously generated orders . the method as defined in claim cavalier , wherein input ##ting the automatic ordering condition further comprises drawing up an automatic trade table , where an automatic trade order is generated from the automatic trade table . the method as defined in claim 8 , wherein the automatic trade condition in the process ( b ) includes a target profit rate , and the process ( f ) further [MASK] calculating a profit rate from the completed contracts before repeating the process ( e ) ; comparing the calculated profit with the target profit rate ; and the computer system stopping the automatic trading if the target profit is obtained . an automatic ordering system of [MASK] , the system including a user computer system connect ##able to a computer system at a stock exchange through a data communication network , [MASK] system comprising : a user interface at the user computer system for the user to input an automatic trade condition needed a memory device for storing basic information data including an item 11th of a stock and an account number of a stock holder input to the computer system through the user interface ; a trade condition control module for storing an automatic stock [MASK] condition based on which a selling order including price and quantity and a purchase order including price and quantity for trade of the stock are determined [SEP]\n",
            "I0811 12:00:06.690949 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 3274 2291 1997 4366 2403 1010 16726 1996 2012 2560 2028 11486 103 28578 11187 1996 4431 4175 2005 1996 2012 2560 2028 2951 3796 1999 3433 2000 4526 1996 3589 103 1012 102 2897 1025 1006 1039 1007 2043 3202 2044 1996 3988 2344 2003 11016 1010 1996 3274 2291 8073 1010 2302 2019 8830 2011 1996 5310 1010 11717 1998 6885 2119 1037 2047 5271 2344 1998 1037 2047 5309 2344 2083 1996 2951 4807 2897 2429 2000 1996 6882 3119 4650 1010 1996 5271 2344 2108 2012 1037 3976 3020 2084 1996 11016 3976 103 1996 3988 2344 1998 1996 5309 2344 2108 2012 1037 3976 2896 2084 1996 11016 3976 2005 103 3988 2344 1025 1006 103 1007 103 3202 2044 2028 1997 1996 4397 2872 5271 1998 5309 4449 2003 11016 1010 103 3274 2291 8073 1010 2302 2019 8830 2011 1996 5310 1010 11717 1998 6885 1037 2047 5309 2344 1998 1037 2047 5271 2344 2005 3119 2429 2000 1996 6882 3119 4650 1010 1996 5271 7406 2108 103 1037 3976 3020 2084 1996 103 11016 3976 1998 1996 5309 103 2108 2012 1037 3976 2896 2084 1996 3130 11016 3976 1025 1998 1006 1041 103 1996 3274 2291 15192 1996 2832 1006 1040 1007 1012 1996 4118 2004 4225 1999 4366 1022 1010 16726 1996 6882 3119 4650 19421 4855 1998 5309 2344 7597 3445 2030 10548 2011 1037 4964 3815 2013 1996 3130 7013 4449 1012 1996 4118 2004 4225 1999 4366 1022 1010 16726 1996 6882 3119 4650 19421 4855 1998 5309 2344 7597 3445 2030 10548 2011 1037 4964 3446 2013 1996 3130 7013 4449 1012 1996 4118 2004 4225 1999 4366 28778 1010 16726 7953 3436 1996 6882 13063 4650 2582 8681 5059 2039 2019 6882 3119 2795 1010 2073 2019 6882 3119 2344 2003 7013 2013 1996 6882 3119 2795 1012 1996 4118 2004 4225 1999 4366 1022 1010 16726 1996 6882 3119 4650 1999 1996 2832 1006 1038 1007 2950 1037 4539 5618 3446 1010 1998 1996 2832 1006 1042 1007 2582 103 20177 1037 5618 3446 2013 1996 2949 8311 2077 15192 1996 2832 1006 1041 1007 1025 13599 1996 10174 5618 2007 1996 4539 5618 3446 1025 1998 1996 3274 2291 7458 1996 6882 6202 2065 1996 4539 5618 2003 4663 1012 2019 6882 13063 2291 1997 103 1010 1996 2291 2164 1037 5310 3274 2291 7532 3085 2000 1037 3274 2291 2012 1037 4518 3863 2083 1037 2951 4807 2897 1010 103 2291 9605 1024 1037 5310 8278 2012 1996 5310 3274 2291 2005 1996 5310 2000 7953 2019 6882 3119 4650 2734 1037 3638 5080 2005 23977 3937 2592 2951 2164 2019 8875 6252 1997 1037 4518 1998 2019 4070 2193 1997 1037 4518 9111 7953 2000 1996 3274 2291 2083 1996 5310 8278 1025 1037 3119 4650 2491 11336 2005 23977 2019 6882 4518 103 4650 2241 2006 2029 1037 4855 2344 2164 3976 1998 11712 1998 1037 5309 2344 2164 3976 1998 11712 2005 3119 1997 1996 4518 2024 4340 102\n",
            "I0811 12:00:06.691498 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.692109 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.692272 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 14 33 99 117 122 124 139 174 176 182 188 203 284 347 369 394 419 440 452 484\n",
            "I0811 12:00:06.692546 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 4297 5371 2005 1996 1040 2043 1996 2344 2012 3130 2344 1007 1022 8681 1996 15768 1996 1025 3642 3119\n",
            "I0811 12:00:06.692716 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.693022 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.694283 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.694613 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the semiconductor device according to claim 2 , wherein said shift register and / or said first terminal is used while sharing with a terminal for use during a normal operation . a semiconductor device including a liquid crystal driving circuit , said liquid crystal driving circuit comprising [MASK] digital functional unit , an analog functional unit , and a shift register for functional ##ly dividing said digital functional unit and said analog functional unit from each other and a first terminal coupled to the shift register , at [MASK] last stage of the digital functional unit , for controlling a test of said digital functional unit externally of said li ##gui ##d crystal driving circuit independently of said analog functional unit and a second terminal for controlling a test of said analog functional unit externally of said liquid crystal driving circuit independently of said digital functional unit . the semiconductor device according to claim 5 , wherein said digital functional unit includes a display controller and a display data storage ram , said analog functional unit includes a gr ##ada ##tion voltage generating circuit [MASK] a gr ##ada ##tion voltage selecting circuit , and said device includes hold means for holding an output of said display data storage ram , reads data held in said hold means to outside of said liquid crystal driving circuit through a data output terminal and sets pre ##de [MASK] ##mined data in said hold means from outside of said liquid crystal driving [MASK] through [MASK] shift register . [SEP] least one memory array comprising : [MASK] plurality [MASK] word lines ; [MASK] plurality of bit lines ; a plurality of source lines ; and a plurality of non ##vo ##lat ##ile memory cells , each of at least a subset of the plurality of memory cells having a first terminal connected to one of the plurality of word lines , a [MASK] terminal connected to one of the plurality of bit lines , and a third terminal connected to one of the plurality of source lines , at least one of the memory cells comprising : a bipolar program ##mable storage element operative to store a logic state of the memory cell , a first terminal of the bipolar [MASK] ##mable storage element connecting to a first line of a corresponding bit line / source line pair ; and [MASK] metal - oxide - semiconductor device including first [MASK] second source / drains and a gate , the first source / drain being connected [MASK] [MASK] second terminal of [MASK] bipolar program ##mable storage element , the second [MASK] / drain connecting to a second line of the corresponding bit line / source line pair , and the gate [MASK] to a corresponding one of the word lines ; wherein [MASK] any subset of at least four memory cells connected to a same bit line , each of the four memory cells being adjacent to one another , at least two adjacent memory cells in the subset share a same word line and [SEP]\n",
            "I0811 12:00:06.694884 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 20681 5080 2429 2000 4366 1016 1010 16726 2056 5670 4236 1998 1013 2030 2056 2034 5536 2003 2109 2096 6631 2007 1037 5536 2005 2224 2076 1037 3671 3169 1012 1037 20681 5080 2164 1037 6381 6121 4439 4984 1010 2056 6381 6121 4439 4984 9605 103 3617 8360 3131 1010 2019 11698 8360 3131 1010 1998 1037 5670 4236 2005 8360 2135 16023 2056 3617 8360 3131 1998 2056 11698 8360 3131 2013 2169 2060 1998 1037 2034 5536 11211 2000 1996 5670 4236 1010 2012 103 2197 2754 1997 1996 3617 8360 3131 1010 2005 9756 1037 3231 1997 2056 3617 8360 3131 27223 1997 2056 5622 25698 2094 6121 4439 4984 9174 1997 2056 11698 8360 3131 1998 1037 2117 5536 2005 9756 1037 3231 1997 2056 11698 8360 3131 27223 1997 2056 6381 6121 4439 4984 9174 1997 2056 3617 8360 3131 1012 1996 20681 5080 2429 2000 4366 1019 1010 16726 2056 3617 8360 3131 2950 1037 4653 11486 1998 1037 4653 2951 5527 8223 1010 2056 11698 8360 3131 2950 1037 24665 8447 3508 10004 11717 4984 103 1037 24665 8447 3508 10004 17739 4984 1010 1998 2056 5080 2950 2907 2965 2005 3173 2019 6434 1997 2056 4653 2951 5527 8223 1010 9631 2951 2218 1999 2056 2907 2965 2000 2648 1997 2056 6381 6121 4439 4984 2083 1037 2951 6434 5536 1998 4520 3653 3207 103 25089 2951 1999 2056 2907 2965 2013 2648 1997 2056 6381 6121 4439 103 2083 103 5670 4236 1012 102 2560 2028 3638 9140 9605 1024 103 29018 103 2773 3210 1025 103 29018 1997 2978 3210 1025 1037 29018 1997 3120 3210 1025 1998 1037 29018 1997 2512 6767 20051 9463 3638 4442 1010 2169 1997 2012 2560 1037 16745 1997 1996 29018 1997 3638 4442 2383 1037 2034 5536 4198 2000 2028 1997 1996 29018 1997 2773 3210 1010 1037 103 5536 4198 2000 2028 1997 1996 29018 1997 2978 3210 1010 1998 1037 2353 5536 4198 2000 2028 1997 1996 29018 1997 3120 3210 1010 2012 2560 2028 1997 1996 3638 4442 9605 1024 1037 29398 2565 24088 5527 5783 12160 2000 3573 1037 7961 2110 1997 1996 3638 3526 1010 1037 2034 5536 1997 1996 29398 103 24088 5527 5783 7176 2000 1037 2034 2240 1997 1037 7978 2978 2240 1013 3120 2240 3940 1025 1998 103 3384 1011 15772 1011 20681 5080 2164 2034 103 2117 3120 1013 18916 1998 1037 4796 1010 1996 2034 3120 1013 12475 2108 4198 103 103 2117 5536 1997 103 29398 2565 24088 5527 5783 1010 1996 2117 103 1013 12475 7176 2000 1037 2117 2240 1997 1996 7978 2978 2240 1013 3120 2240 3940 1010 1998 1996 4796 103 2000 1037 7978 2028 1997 1996 2773 3210 1025 16726 103 2151 16745 1997 2012 2560 2176 3638 4442 4198 2000 1037 2168 2978 2240 1010 2169 1997 1996 2176 3638 4442 2108 5516 2000 2028 2178 1010 2012 2560 2048 5516 3638 4442 1999 1996 16745 3745 1037 2168 2773 2240 1998 102\n",
            "I0811 12:00:06.695100 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.695306 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.695398 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 31 49 90 186 236 250 252 263 265 269 319 377 397 406 422 423 427 436 457 468\n",
            "I0811 12:00:06.695478 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 3169 1037 1037 1998 3334 4984 2056 1037 1997 1037 2117 2565 1037 1998 2000 1037 1996 3120 7176 2005\n",
            "I0811 12:00:06.695563 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.695633 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.696473 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.696806 140302422521728 create_pretraining_data.py:151] tokens: [CLS] a first integrated circuit device that is coupled to a first external signal line , the method comprising : receiving a termination control signal from a memory controller ; switch ##ably coupling a first termination load element [MASK] the first external signal line if the termination control signal indicates that a data signal to be transmitted on the first external [MASK] line is directed to the first integrated circuit device ; and switch ##ably coupling a second termination load element to the first external signal line if ##uda termination control signal indicates that a data signal to be transmitted on the first external signal line is directed to another [MASK] circuit device . the method of claim 15 further comprising receiving the termination control signal a pre ##de ##ter ##mined amount of time prior to transmission of the first data signal on the first external signal line [MASK] a method of operation within a memory controller , wherein a data path is coupled between the memory controller and a first memory module , the method comprising : output ##ting a control signal having a first state to a first memory module , the control [MASK] having the first state to enable a memory [MASK] within the first memory module [MASK] couple miocene first plurality of termination load elements to the [MASK] path if data signals are to be received within the first memory [MASK] via the data path ; and output ##ting the control signal having the second state to the first memory module , the control signal having the second state to enable [MASK] memory device within the first memory module to couple a second plurality of termination load elements to the data path if data signals are to be received within a second memory [SEP] the method of claim 17 wherein the control signal comprises first and second component signals , and wherein output ##ting the control signal to [MASK] first memory module in the first state comprises output ##ting the first component signal to the first memory module in a first logic state and output ##ting the second component signal to the first memory module in a second logic state . the method of claim 17 further comprising determining whether the data signals are to be transmitted to the first memory module or the second memory module according to an address value received from an external device . the method of claim 17 further comprising receiving information from a [MASK] - volatile storage device disposed on the first memory module , the information indicating that the first memory device includes the first plurality of termination load elements and the second plurality of termination load elements . the method [MASK] claim 17 further comprising output ##ting an instruction [MASK] the first memory device in association with an imp ##edance selection value , the instruction ins ##tructing the first memory device to [MASK] the imp ##edance selection value within a configuration circuit of the first memory device to establish [MASK] imp ##edance value for the first termination elements . [SEP]\n",
            "I0811 12:00:06.697049 140302422521728 create_pretraining_data.py:161] input_ids: 101 1037 2034 6377 4984 5080 2008 2003 11211 2000 1037 2034 6327 4742 2240 1010 1996 4118 9605 1024 4909 1037 18287 2491 4742 2013 1037 3638 11486 1025 6942 8231 19780 1037 2034 18287 7170 5783 103 1996 2034 6327 4742 2240 2065 1996 18287 2491 4742 7127 2008 1037 2951 4742 2000 2022 11860 2006 1996 2034 6327 103 2240 2003 2856 2000 1996 2034 6377 4984 5080 1025 1998 6942 8231 19780 1037 2117 18287 7170 5783 2000 1996 2034 6327 4742 2240 2065 14066 18287 2491 4742 7127 2008 1037 2951 4742 2000 2022 11860 2006 1996 2034 6327 4742 2240 2003 2856 2000 2178 103 4984 5080 1012 1996 4118 1997 4366 2321 2582 9605 4909 1996 18287 2491 4742 1037 3653 3207 3334 25089 3815 1997 2051 3188 2000 6726 1997 1996 2034 2951 4742 2006 1996 2034 6327 4742 2240 103 1037 4118 1997 3169 2306 1037 3638 11486 1010 16726 1037 2951 4130 2003 11211 2090 1996 3638 11486 1998 1037 2034 3638 11336 1010 1996 4118 9605 1024 6434 3436 1037 2491 4742 2383 1037 2034 2110 2000 1037 2034 3638 11336 1010 1996 2491 103 2383 1996 2034 2110 2000 9585 1037 3638 103 2306 1996 2034 3638 11336 103 3232 26926 2034 29018 1997 18287 7170 3787 2000 1996 103 4130 2065 2951 7755 2024 2000 2022 2363 2306 1996 2034 3638 103 3081 1996 2951 4130 1025 1998 6434 3436 1996 2491 4742 2383 1996 2117 2110 2000 1996 2034 3638 11336 1010 1996 2491 4742 2383 1996 2117 2110 2000 9585 103 3638 5080 2306 1996 2034 3638 11336 2000 3232 1037 2117 29018 1997 18287 7170 3787 2000 1996 2951 4130 2065 2951 7755 2024 2000 2022 2363 2306 1037 2117 3638 102 1996 4118 1997 4366 2459 16726 1996 2491 4742 8681 2034 1998 2117 6922 7755 1010 1998 16726 6434 3436 1996 2491 4742 2000 103 2034 3638 11336 1999 1996 2034 2110 8681 6434 3436 1996 2034 6922 4742 2000 1996 2034 3638 11336 1999 1037 2034 7961 2110 1998 6434 3436 1996 2117 6922 4742 2000 1996 2034 3638 11336 1999 1037 2117 7961 2110 1012 1996 4118 1997 4366 2459 2582 9605 12515 3251 1996 2951 7755 2024 2000 2022 11860 2000 1996 2034 3638 11336 2030 1996 2117 3638 11336 2429 2000 2019 4769 3643 2363 2013 2019 6327 5080 1012 1996 4118 1997 4366 2459 2582 9605 4909 2592 2013 1037 103 1011 20606 5527 5080 21866 2006 1996 2034 3638 11336 1010 1996 2592 8131 2008 1996 2034 3638 5080 2950 1996 2034 29018 1997 18287 7170 3787 1998 1996 2117 29018 1997 18287 7170 3787 1012 1996 4118 103 4366 2459 2582 9605 6434 3436 2019 7899 103 1996 2034 3638 5080 1999 2523 2007 2019 17727 29605 4989 3643 1010 1996 7899 16021 26310 1996 2034 3638 5080 2000 103 1996 17727 29605 4989 3643 2306 1037 9563 4984 1997 1996 2034 3638 5080 2000 5323 103 17727 29605 3643 2005 1996 2034 18287 3787 1012 102\n",
            "I0811 12:00:06.697258 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.794944 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.795337 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 38 61 88 110 148 153 195 204 210 212 221 234 250 265 322 413 452 461 484 501\n",
            "I0811 12:00:06.795480 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 2000 4742 1996 6377 1012 2306 4742 5080 2000 1037 2951 11336 2000 1996 1996 2512 1997 2000 3573 2019\n",
            "I0811 12:00:06.795585 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.795682 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.797114 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.797526 140302422521728 create_pretraining_data.py:151] tokens: [CLS] of 1 - 4 , n being identical or different for different rings , the substituted moi ##eti ##es x being identical or different when n & # x ##3 ##e ; 1 ; y 1 and y 2 are & # x ##20 ##14 ; r , & # x ##20 ##14 ; o & # x ##20 ##14 ; r , & # x ##20 [MASK] ; co & # x ##20 ##14 ; r , & # x ##20 ##14 ; o ##co & # x ##20 ##14 ; r , & # x ##20 ##14 ; co ##o & # x ##20 noisy ; r , or & [MASK] x ##20 ##14 ; ( och 2 ch 2 ) n [MASK] ch 3 , r representing a c1 - 12 linear or branched al ##ky ##l , n ##1 being an integer of 1 - 5 , y 1 and y 2 being identical or different . the liquid crystal medium composition for use in liquid crystal display as claimed in claim 1 , wherein the stabilize ##r comprises [MASK] least one compound represented by [MASK] following formula : wherein r 1 represents at least one of c1 - 9 linear or branched al ##ky ##l , n being an integer of 1 - 4 , the substituted moi ##eti ##es r 1 being identical or different when n & # x ##3 ##e ; 1 ; r 2 represents a c1 - 36 linear or branched al ##ky ##l ; and l is a carbon - carbon single [MASK] [MASK] & # x ##20 ##14 [MASK] o [SEP] the liquid crystal medium composition for use in liquid crystal display [MASK] claimed in claim 2 , wherein the polymer ##iza [MASK] mono ##mers are selected from two or three of the following structural formulas : wherein in formula v , x is & # x ##20 ##14 ; f or & # x ##20 ##14 ; [MASK] ; in formulas vi and vii , z is & # x ##20 ##14 ; o & # x ##20 ##14 ; , & # x ##20 ##14 ; co ##o ش # x ##20 ##14 ; , & # x ##20 ##14 ; o ##co & # x ##20 ##14 ; , [MASK] # x ##20 ##14 ; ch 2 o & # x ##20 [MASK] ; , & # x ##20 ##14 ; och [MASK] o & # x ##20 ##14 ; , & # x ##20 ##14 ; o ( ch 2 ) 2 o & [MASK] x ##20 ##14 ; , & # x ##20 ##14 ; co ##ch 2 & # x ##20 ##14 ; , methyl ##en ##yl , & # x ##20 ##14 ; c & # x ##22 ##6 ##1 ; c & # x ##20 ##14 ; , any one of said two or three of the polymer ##iza ##ble mono ##mers having a mo ##lar ratio less than or equal to 98 % based on the total weight . [SEP]\n",
            "I0811 12:00:06.797829 140302422521728 create_pretraining_data.py:161] input_ids: 101 1997 1015 1011 1018 1010 1050 2108 7235 2030 2367 2005 2367 7635 1010 1996 17316 25175 20624 2229 1060 2108 7235 2030 2367 2043 1050 1004 1001 1060 2509 2063 1025 1015 1025 1061 1015 1998 1061 1016 2024 1004 1001 1060 11387 16932 1025 1054 1010 1004 1001 1060 11387 16932 1025 1051 1004 1001 1060 11387 16932 1025 1054 1010 1004 1001 1060 11387 103 1025 2522 1004 1001 1060 11387 16932 1025 1054 1010 1004 1001 1060 11387 16932 1025 1051 3597 1004 1001 1060 11387 16932 1025 1054 1010 1004 1001 1060 11387 16932 1025 2522 2080 1004 1001 1060 11387 20810 1025 1054 1010 2030 1004 103 1060 11387 16932 1025 1006 28166 1016 10381 1016 1007 1050 103 10381 1017 1010 1054 5052 1037 27723 1011 2260 7399 2030 21648 2632 4801 2140 1010 1050 2487 2108 2019 16109 1997 1015 1011 1019 1010 1061 1015 1998 1061 1016 2108 7235 2030 2367 1012 1996 6381 6121 5396 5512 2005 2224 1999 6381 6121 4653 2004 3555 1999 4366 1015 1010 16726 1996 27790 2099 8681 103 2560 2028 7328 3421 2011 103 2206 5675 1024 16726 1054 1015 5836 2012 2560 2028 1997 27723 1011 1023 7399 2030 21648 2632 4801 2140 1010 1050 2108 2019 16109 1997 1015 1011 1018 1010 1996 17316 25175 20624 2229 1054 1015 2108 7235 2030 2367 2043 1050 1004 1001 1060 2509 2063 1025 1015 1025 1054 1016 5836 1037 27723 1011 4029 7399 2030 21648 2632 4801 2140 1025 1998 1048 2003 1037 6351 1011 6351 2309 103 103 1004 1001 1060 11387 16932 103 1051 102 1996 6381 6121 5396 5512 2005 2224 1999 6381 6121 4653 103 3555 1999 4366 1016 1010 16726 1996 17782 21335 103 18847 16862 2024 3479 2013 2048 2030 2093 1997 1996 2206 8332 25814 1024 16726 1999 5675 1058 1010 1060 2003 1004 1001 1060 11387 16932 1025 1042 2030 1004 1001 1060 11387 16932 1025 103 1025 1999 25814 6819 1998 8890 1010 1062 2003 1004 1001 1060 11387 16932 1025 1051 1004 1001 1060 11387 16932 1025 1010 1004 1001 1060 11387 16932 1025 2522 2080 1283 1001 1060 11387 16932 1025 1010 1004 1001 1060 11387 16932 1025 1051 3597 1004 1001 1060 11387 16932 1025 1010 103 1001 1060 11387 16932 1025 10381 1016 1051 1004 1001 1060 11387 103 1025 1010 1004 1001 1060 11387 16932 1025 28166 103 1051 1004 1001 1060 11387 16932 1025 1010 1004 1001 1060 11387 16932 1025 1051 1006 10381 1016 1007 1016 1051 1004 103 1060 11387 16932 1025 1010 1004 1001 1060 11387 16932 1025 2522 2818 1016 1004 1001 1060 11387 16932 1025 1010 25003 2368 8516 1010 1004 1001 1060 11387 16932 1025 1039 1004 1001 1060 19317 2575 2487 1025 1039 1004 1001 1060 11387 16932 1025 1010 2151 2028 1997 2056 2048 2030 2093 1997 1996 17782 21335 3468 18847 16862 2383 1037 9587 8017 6463 2625 2084 2030 5020 2000 5818 1003 2241 2006 1996 2561 3635 1012 102\n",
            "I0811 12:00:06.798054 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.798262 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.798354 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 68 107 113 125 139 184 190 264 265 271 285 295 303 331 363 377 385 398 408 431\n",
            "I0811 12:00:06.798437 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 16932 16932 1001 2487 4801 2012 1996 5416 1010 1025 2004 3468 2093 27166 1004 3597 1004 16932 1016 1001\n",
            "I0811 12:00:06.798520 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.798589 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:06.806521 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.806890 140302422521728 create_pretraining_data.py:151] tokens: [CLS] , wherein the intake trumpet or the housing or the inlet chamber housing or an intake bottom of the housing through which the intake trumpet passes or an outlet bottom of the housing through which the exhaust pipe passes are designed as a double wall with an insulation insert . the mu ##ffle ##r according [MASK] claim [MASK] , wherein the inlet chamber is sealed by sold ##ered connections or by welded connections which surround the intake trumpet or each catalytic convert ##er element or the inlet chamber with respect to the housing in a ring shape . a mu ##ffle [MASK] having an integrated catalytic convert ##er for an exhaust line of an internal combustion engine [MASK] : ( a ) a housing having [MASK] input end , an output end , an intake trumpet on the input end , and an exhaust pipe on the output end , said intake trumpet having [MASK] section containing an intake trumpet outlet end and forming an inlet chamber communicating with said [MASK] trumpet outlet end ; and ( b [MASK] at least two catalytic convert ##er elements arranged in the housing having parallel flow through said at least two catalytic [MASK] ##er elements [MASK] each catalytic convert ##er element comprising one catalytic convert ##er element inlet end in the inlet chamber and one catalytic convert ##er element outlet end in an outlet chamber , said inlet chamber being sealed off from crazed outside and from the intake trumpet and said at least two catalytic convert ##er elements to the extent that during operation of the internal combustion engine , exhaust gas entering the inlet chamber through the intake trumpet leaves the inlet chamber only through said at least two catalytic convert ##er elements . a locking device for a steering shaft having a gear of a reduction gear mechanism axial ##ly fastened there ##on to substantially prevent movement between the steering shaft and the gear , comprising : a experimentation part which slides into a housing of the steering shaft ; and an engagement part formed on a side face of the gear , which engages with an end portion of the locking part when the locking part slides into the housing [MASK] wherein [SEP] standard 20 mesh screen and retained on a u . s . standard 100 mesh screen . the process for recycling super ##ab ##sor ##ben ##t polymer fines in the production [MASK] super [MASK] ##sor ##ben ##t polymer gel of claim 15 further comprising the steps to make a super ##ab ##sor ##ben ##t polymer composition wherein the steps comprise coating the super ##ab ##sor ##ben ##t polymer part ##iculate with [MASK] surface cross ##link ##ing agent and surface additive ##s , heating said [MASK] polymer , separating said dried super ##ab ##sor ##ben ##t polymer part ##iculate [MASK] a portion having a desired particle size from about 150 & # x ##3 ##bc ; m to about 850 & # x ##3 ##bc ; m as measured by screening through a u . s . [SEP]\n",
            "I0811 12:00:06.807139 140302422521728 create_pretraining_data.py:161] input_ids: 101 1010 16726 1996 13822 9368 2030 1996 3847 2030 1996 15824 4574 3847 2030 2019 13822 3953 1997 1996 3847 2083 2029 1996 13822 9368 5235 2030 2019 13307 3953 1997 1996 3847 2083 2029 1996 15095 8667 5235 2024 2881 2004 1037 3313 2813 2007 2019 25710 19274 1012 1996 14163 18142 2099 2429 103 4366 103 1010 16726 1996 15824 4574 2003 10203 2011 2853 6850 7264 2030 2011 29014 7264 2029 15161 1996 13822 9368 2030 2169 26244 10463 2121 5783 2030 1996 15824 4574 2007 4847 2000 1996 3847 1999 1037 3614 4338 1012 1037 14163 18142 103 2383 2019 6377 26244 10463 2121 2005 2019 15095 2240 1997 2019 4722 16513 3194 103 1024 1006 1037 1007 1037 3847 2383 103 7953 2203 1010 2019 6434 2203 1010 2019 13822 9368 2006 1996 7953 2203 1010 1998 2019 15095 8667 2006 1996 6434 2203 1010 2056 13822 9368 2383 103 2930 4820 2019 13822 9368 13307 2203 1998 5716 2019 15824 4574 20888 2007 2056 103 9368 13307 2203 1025 1998 1006 1038 103 2012 2560 2048 26244 10463 2121 3787 5412 1999 1996 3847 2383 5903 4834 2083 2056 2012 2560 2048 26244 103 2121 3787 103 2169 26244 10463 2121 5783 9605 2028 26244 10463 2121 5783 15824 2203 1999 1996 15824 4574 1998 2028 26244 10463 2121 5783 13307 2203 1999 2019 13307 4574 1010 2056 15824 4574 2108 10203 2125 2013 28343 2648 1998 2013 1996 13822 9368 1998 2056 2012 2560 2048 26244 10463 2121 3787 2000 1996 6698 2008 2076 3169 1997 1996 4722 16513 3194 1010 15095 3806 5738 1996 15824 4574 2083 1996 13822 9368 3727 1996 15824 4574 2069 2083 2056 2012 2560 2048 26244 10463 2121 3787 1012 1037 14889 5080 2005 1037 9602 9093 2383 1037 6718 1997 1037 7312 6718 7337 26819 2135 24009 2045 2239 2000 12381 4652 2929 2090 1996 9602 9093 1998 1996 6718 1010 9605 1024 1037 21470 2112 2029 14816 2046 1037 3847 1997 1996 9602 9093 1025 1998 2019 8147 2112 2719 2006 1037 2217 2227 1997 1996 6718 1010 2029 24255 2007 2019 2203 4664 1997 1996 14889 2112 2043 1996 14889 2112 14816 2046 1996 3847 103 16726 102 3115 2322 20437 3898 1998 6025 2006 1037 1057 1012 1055 1012 3115 2531 20437 3898 1012 1996 2832 2005 17874 3565 7875 21748 10609 2102 17782 21892 1999 1996 2537 103 3565 103 21748 10609 2102 17782 21500 1997 4366 2321 2582 9605 1996 4084 2000 2191 1037 3565 7875 21748 10609 2102 17782 5512 16726 1996 4084 15821 18898 1996 3565 7875 21748 10609 2102 17782 2112 24153 2007 103 3302 2892 13767 2075 4005 1998 3302 29167 2015 1010 10808 2056 103 17782 1010 14443 2056 9550 3565 7875 21748 10609 2102 17782 2112 24153 103 1037 4664 2383 1037 9059 10811 2946 2013 2055 5018 1004 1001 1060 2509 9818 1025 1049 2000 2055 15678 1004 1001 1060 2509 9818 1025 1049 2004 7594 2011 11326 2083 1037 1057 1012 1055 1012 102\n",
            "I0811 12:00:06.807358 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.905139 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.905385 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 56 58 102 118 126 137 155 171 179 200 203 241 287 329 372 406 408 446 459 473\n",
            "I0811 12:00:06.905487 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 2000 1015 2099 9605 2019 2006 1037 13822 1007 10463 1010 1996 2560 14889 1010 1997 7875 1037 15026 2046\n",
            "I0811 12:00:06.905587 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.905680 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.906607 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.906967 140302422521728 create_pretraining_data.py:151] tokens: [CLS] a method according to claim 1 , wherein translating in the gateway comprises translating a virtual destination identity comprised in the primary zone - data - frame to an identity of the primary v ##lan in the primary v ##lan - data - frame , for data conveyed from the second network [MASK] the first network . [SEP] , thereby the data are stored in the memory unit ; when the current of the detection current input end read out by the first switching element is smaller than the current of the reference current end read out by the second switching element , a voltage of the negative input end of the com ##para ##tor is smaller [MASK] a voltage of the positive input [MASK] of the com ##para ##tor , the output end of the com ##para ##tor outputs a high level , thereby no data is stored in the memory unit . the current detection method , as recited in claim 6 , wherein when [MASK] current of the detection current input end read out by the first switching element [MASK] larger than the current of the reference current end read out by the second switching element , a voltage of the negative input end of the com ##para ##tor is larger than a voltage of ##isa positive input end of the [MASK] ##para ##tor , the output end of the com ##para ##tor outputs a [MASK] level , thereby [MASK] data are stored in the memory unit ; when the current of the detection current input end read out by the first switching element is smaller than the current of the reference current end cam [MASK] by the second switching [MASK] , a [MASK] of the negative input end of the com ##para ##tor is smaller than a voltage of the positive input end of the com ##para ##tor , the output end of the com ##para ##tor outputs a high level , thereby no data is stored in the memory unit . a current detection circuit , comprising a detection current input end , a reference current end , a first field effect trans ##isto ##r connected with said detection current input end , [MASK] second field effect trans ##isto ##r connected with said reference current end , an operational amplifier , a com ##para ##tor , a reference voltage end , an output end and a ground end , wherein said detection current input end is connected with a memory unit of a memory , wherein said [MASK] current input end , a drain electrode of said first field effect trans ##isto ##r , a positive input end of said operational amplifier and a negative input end of said com ##para ##tor are connected with each other , said reference current end , a drain electrode of said second field effect trans ##isto ##r [MASK] a positive input end of said com ##para ##tor are connected with each other , [MASK] source electrode of said first field effect trans ##isto [MASK] and a source electrode [SEP]\n",
            "I0811 12:00:06.907214 140302422521728 create_pretraining_data.py:161] input_ids: 101 1037 4118 2429 2000 4366 1015 1010 16726 22969 1999 1996 11909 8681 22969 1037 7484 7688 4767 11539 1999 1996 3078 4224 1011 2951 1011 4853 2000 2019 4767 1997 1996 3078 1058 5802 1999 1996 3078 1058 5802 1011 2951 1011 4853 1010 2005 2951 21527 2013 1996 2117 2897 103 1996 2034 2897 1012 102 1010 8558 1996 2951 2024 8250 1999 1996 3638 3131 1025 2043 1996 2783 1997 1996 10788 2783 7953 2203 3191 2041 2011 1996 2034 11991 5783 2003 3760 2084 1996 2783 1997 1996 4431 2783 2203 3191 2041 2011 1996 2117 11991 5783 1010 1037 10004 1997 1996 4997 7953 2203 1997 1996 4012 28689 4263 2003 3760 103 1037 10004 1997 1996 3893 7953 103 1997 1996 4012 28689 4263 1010 1996 6434 2203 1997 1996 4012 28689 4263 27852 1037 2152 2504 1010 8558 2053 2951 2003 8250 1999 1996 3638 3131 1012 1996 2783 10788 4118 1010 2004 24843 1999 4366 1020 1010 16726 2043 103 2783 1997 1996 10788 2783 7953 2203 3191 2041 2011 1996 2034 11991 5783 103 3469 2084 1996 2783 1997 1996 4431 2783 2203 3191 2041 2011 1996 2117 11991 5783 1010 1037 10004 1997 1996 4997 7953 2203 1997 1996 4012 28689 4263 2003 3469 2084 1037 10004 1997 14268 3893 7953 2203 1997 1996 103 28689 4263 1010 1996 6434 2203 1997 1996 4012 28689 4263 27852 1037 103 2504 1010 8558 103 2951 2024 8250 1999 1996 3638 3131 1025 2043 1996 2783 1997 1996 10788 2783 7953 2203 3191 2041 2011 1996 2034 11991 5783 2003 3760 2084 1996 2783 1997 1996 4431 2783 2203 11503 103 2011 1996 2117 11991 103 1010 1037 103 1997 1996 4997 7953 2203 1997 1996 4012 28689 4263 2003 3760 2084 1037 10004 1997 1996 3893 7953 2203 1997 1996 4012 28689 4263 1010 1996 6434 2203 1997 1996 4012 28689 4263 27852 1037 2152 2504 1010 8558 2053 2951 2003 8250 1999 1996 3638 3131 1012 1037 2783 10788 4984 1010 9605 1037 10788 2783 7953 2203 1010 1037 4431 2783 2203 1010 1037 2034 2492 3466 9099 20483 2099 4198 2007 2056 10788 2783 7953 2203 1010 103 2117 2492 3466 9099 20483 2099 4198 2007 2056 4431 2783 2203 1010 2019 6515 22686 1010 1037 4012 28689 4263 1010 1037 4431 10004 2203 1010 2019 6434 2203 1998 1037 2598 2203 1010 16726 2056 10788 2783 7953 2203 2003 4198 2007 1037 3638 3131 1997 1037 3638 1010 16726 2056 103 2783 7953 2203 1010 1037 12475 28688 1997 2056 2034 2492 3466 9099 20483 2099 1010 1037 3893 7953 2203 1997 2056 6515 22686 1998 1037 4997 7953 2203 1997 2056 4012 28689 4263 2024 4198 2007 2169 2060 1010 2056 4431 2783 2203 1010 1037 12475 28688 1997 2056 2117 2492 3466 9099 20483 2099 103 1037 3893 7953 2203 1997 2056 4012 28689 4263 2024 4198 2007 2169 2060 1010 103 3120 28688 1997 2056 2034 2492 3466 9099 20483 103 1998 1037 3120 28688 102\n",
            "I0811 12:00:06.907431 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.907634 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:06.907746 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 53 118 125 168 183 219 225 239 243 278 279 284 287 349 369 423 480 494 496 506\n",
            "I0811 12:00:06.907827 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 2000 2084 2203 1996 2003 1996 4012 2659 1996 3191 2041 5783 10004 1037 1037 10788 1998 2060 1037 2099\n",
            "I0811 12:00:06.907914 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:06.907986 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0811 12:00:06.908859 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:06.909177 140302422521728 create_pretraining_data.py:151] tokens: [CLS] a semiconductor integrated circuit testing device according to claim 8 , wherein the pre ##de ##ter ##mined operation pattern signal has a plurality of operation steps , and the measure means and the [MASK] means carry out the measure and the comparison every operation step of the pre ##de ##ter ##mined operation pattern signal , respectively . [SEP] time for which amounts of currents caused to flow through the [MASK] of resist ##ors are stable in each of the operation steps of the pre ##de ##ter ##mined operation pattern . a semiconductor integrated circuit testing device according to claim 11 , further comprising specify ##ing [MASK] for causing the control signal output means to generate the control signal so that in the operation step of the pre ##de ##ter ##mined [MASK] pattern in which the judgment means judges that the semiconductor integrated circuit is abnormal , the on periods of time of the control [MASK] are allocated to each of the plurality of buffer ##s , respectively , to specify an output abnormal [MASK] of the semiconductor integrated circuit based on the judgment results obtained from the judgment means . a semiconductor integrated circuit testing device according to claim 3 , wherein the comparison means comprises : a first detection means for detecting an [MASK] value among normal values of total sums of [MASK] of currents each obtained every operation step , which are measured by the measure means over all the operation steps of [MASK] pre ##de ##ter ##mined operation pattern in a non - defective sample which is used instead of the semiconductor integrated circuit , has the same functions as the functions of the semiconductor integrated circuit , and is verified in advance [MASK] normally operate based on the input pre ##de [MASK] ##mined operation pattern ; production means for producing a criterion zone having a pre ##de ##ter ##mined width based on the extreme value detected by the first detection means ; ##ttle second detection means for detecting an extreme value among total sums of amounts of currents each obtained every operation step , which are measured [MASK] the measure means over all the operation steps of the pre ##de ##ter ##mined operation pattern signal in the semiconductor integrated circuit ; and [MASK] means for determining whether or not the extreme value detected by the second detection means falls within the criterion zone produced by the production means . a semiconductor integrated circuit testing device according to claim 14 , wherein : the extreme value detected by the first detection means is a maximum current value among the normal values of the total sums of amounts [MASK] currents each ##oed every operation step , which are measured by the measure means over all the operation steps of the pre ##de ##ter ##mined operation pattern signal in the non - defective sample ; and [MASK] semiconductor integrated circuit testing device further comprises setting means for , when the first determining means determines that [MASK] extreme value falls within the criterion zone , setting a current measure range [SEP]\n",
            "I0811 12:00:06.909422 140302422521728 create_pretraining_data.py:161] input_ids: 101 1037 20681 6377 4984 5604 5080 2429 2000 4366 1022 1010 16726 1996 3653 3207 3334 25089 3169 5418 4742 2038 1037 29018 1997 3169 4084 1010 1998 1996 5468 2965 1998 1996 103 2965 4287 2041 1996 5468 1998 1996 7831 2296 3169 3357 1997 1996 3653 3207 3334 25089 3169 5418 4742 1010 4414 1012 102 2051 2005 2029 8310 1997 14731 3303 2000 4834 2083 1996 103 1997 9507 5668 2024 6540 1999 2169 1997 1996 3169 4084 1997 1996 3653 3207 3334 25089 3169 5418 1012 1037 20681 6377 4984 5604 5080 2429 2000 4366 2340 1010 2582 9605 20648 2075 103 2005 4786 1996 2491 4742 6434 2965 2000 9699 1996 2491 4742 2061 2008 1999 1996 3169 3357 1997 1996 3653 3207 3334 25089 103 5418 1999 2029 1996 8689 2965 6794 2008 1996 20681 6377 4984 2003 19470 1010 1996 2006 6993 1997 2051 1997 1996 2491 103 2024 11095 2000 2169 1997 1996 29018 1997 17698 2015 1010 4414 1010 2000 20648 2019 6434 19470 103 1997 1996 20681 6377 4984 2241 2006 1996 8689 3463 4663 2013 1996 8689 2965 1012 1037 20681 6377 4984 5604 5080 2429 2000 4366 1017 1010 16726 1996 7831 2965 8681 1024 1037 2034 10788 2965 2005 25952 2019 103 3643 2426 3671 5300 1997 2561 20571 1997 103 1997 14731 2169 4663 2296 3169 3357 1010 2029 2024 7594 2011 1996 5468 2965 2058 2035 1996 3169 4084 1997 103 3653 3207 3334 25089 3169 5418 1999 1037 2512 1011 28829 7099 2029 2003 2109 2612 1997 1996 20681 6377 4984 1010 2038 1996 2168 4972 2004 1996 4972 1997 1996 20681 6377 4984 1010 1998 2003 20119 1999 5083 103 5373 5452 2241 2006 1996 7953 3653 3207 103 25089 3169 5418 1025 2537 2965 2005 5155 1037 19229 4224 2383 1037 3653 3207 3334 25089 9381 2241 2006 1996 6034 3643 11156 2011 1996 2034 10788 2965 1025 26328 2117 10788 2965 2005 25952 2019 6034 3643 2426 2561 20571 1997 8310 1997 14731 2169 4663 2296 3169 3357 1010 2029 2024 7594 103 1996 5468 2965 2058 2035 1996 3169 4084 1997 1996 3653 3207 3334 25089 3169 5418 4742 1999 1996 20681 6377 4984 1025 1998 103 2965 2005 12515 3251 2030 2025 1996 6034 3643 11156 2011 1996 2117 10788 2965 4212 2306 1996 19229 4224 2550 2011 1996 2537 2965 1012 1037 20681 6377 4984 5604 5080 2429 2000 4366 2403 1010 16726 1024 1996 6034 3643 11156 2011 1996 2034 10788 2965 2003 1037 4555 2783 3643 2426 1996 3671 5300 1997 1996 2561 20571 1997 8310 103 14731 2169 29099 2296 3169 3357 1010 2029 2024 7594 2011 1996 5468 2965 2058 2035 1996 3169 4084 1997 1996 3653 3207 3334 25089 3169 5418 4742 1999 1996 2512 1011 28829 7099 1025 1998 103 20681 6377 4984 5604 5080 2582 8681 4292 2965 2005 1010 2043 1996 2034 12515 2965 16463 2008 103 6034 3643 4212 2306 1996 19229 4224 1010 4292 1037 2783 5468 2846 102\n",
            "I0811 12:00:07.001681 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:07.002254 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:07.002416 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 34 70 106 128 131 155 174 215 224 246 287 296 327 352 377 423 441 444 478 497\n",
            "I0811 12:00:07.002532 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 7831 29018 2965 3207 3169 4742 5536 6034 8310 1996 2000 3334 1037 2011 12515 2034 1997 4663 1996 1996\n",
            "I0811 12:00:07.002684 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:07.002794 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:00:07.004168 140302422521728 create_pretraining_data.py:149] *** Example ***\n",
            "I0811 12:00:07.004605 140302422521728 create_pretraining_data.py:151] tokens: [CLS] the memory system of claim 14 , wherein the second verify read operation includes , driving the first portion of the plurality of word lines with the ground voltage , and driving a remaining word lines with a [MASK] voltage . [SEP] fail is due to memory cells corresponding to the first portion of the plurality of word lines , the memory controller is configured to control the flash memory device to perform an erase operation with respect to memory cells corresponding to the first portion of the plurality of word lines . the memory system of claim 18 , wherein during the erase operation with respect to the memory cells corresponding to the first portion of the plurality of word lines , the first portion of [MASK] plurality of word lines are driven with the ground voltage and a remaining word lines are [MASK] at a floating state . a method of eras ##ing [MASK] flash memory device , the method comprising : generating an erase voltage in response to an ##ᵣ command ; first driving , [MASK] a first time period , a plurality of word lines with [MASK] ground voltage and driving a well of a substrate with an erase voltage , while floating [MASK] and ground select lines ; second driving , after the first time period el ##ap ##ses , a first portion of the plurality of word lines with the [MASK] voltage and a second portion of the plurality of word lines with a voltage greater than or equal to a power supply voltage , the first portion of the plurality of [MASK] lines including a first word line adjacent to the string select line and a second word line adjacent to the ground select line ; determining whether threshold voltage ##s of the memory cells have reached a target threshold voltage ; [MASK] repeating at least the first driving , the second driving and the determining a number of times ##estra the threshold voltage of each of the memory cells is lower than the target threshold voltage if [MASK] threshold voltage of at least one of the memory cells is determined to be greater than the target threshold voltage . a multi ##car ##rier transmitting apparatus comprising : a dividing section that divides transmit data into high - quality transmit data requiring good quality and [MASK] transmit data other than said high - quality transmit data ; a sub ##car ##rier allocation section [MASK] ##mobile ##rang ##es said high - quality transmit data and said [MASK] transmit data such that said high - quality transmit data is allocated to sub ##car ##rier ##s in a vicinity of a center frequency [MASK] a pre ##de ##ter ##mined frequency domain and said ordinary transmit data is allocated to sub ##car ##rier ##s in a vicinity of both ends in the pre ##de ##ter ##mined frequency domain , and that varies in accordance with channel quality a range of sub ##car ##rier ##s to which said high - quality transmit data is allocated and a range [SEP]\n",
            "I0811 12:00:07.004913 140302422521728 create_pretraining_data.py:161] input_ids: 101 1996 3638 2291 1997 4366 2403 1010 16726 1996 2117 20410 3191 3169 2950 1010 4439 1996 2034 4664 1997 1996 29018 1997 2773 3210 2007 1996 2598 10004 1010 1998 4439 1037 3588 2773 3210 2007 1037 103 10004 1012 102 8246 2003 2349 2000 3638 4442 7978 2000 1996 2034 4664 1997 1996 29018 1997 2773 3210 1010 1996 3638 11486 2003 26928 2000 2491 1996 5956 3638 5080 2000 4685 2019 22505 3169 2007 4847 2000 3638 4442 7978 2000 1996 2034 4664 1997 1996 29018 1997 2773 3210 1012 1996 3638 2291 1997 4366 2324 1010 16726 2076 1996 22505 3169 2007 4847 2000 1996 3638 4442 7978 2000 1996 2034 4664 1997 1996 29018 1997 2773 3210 1010 1996 2034 4664 1997 103 29018 1997 2773 3210 2024 5533 2007 1996 2598 10004 1998 1037 3588 2773 3210 2024 103 2012 1037 8274 2110 1012 1037 4118 1997 28500 2075 103 5956 3638 5080 1010 1996 4118 9605 1024 11717 2019 22505 10004 1999 3433 2000 2019 30043 3094 1025 2034 4439 1010 103 1037 2034 2051 2558 1010 1037 29018 1997 2773 3210 2007 103 2598 10004 1998 4439 1037 2092 1997 1037 16305 2007 2019 22505 10004 1010 2096 8274 103 1998 2598 7276 3210 1025 2117 4439 1010 2044 1996 2034 2051 2558 3449 9331 8583 1010 1037 2034 4664 1997 1996 29018 1997 2773 3210 2007 1996 103 10004 1998 1037 2117 4664 1997 1996 29018 1997 2773 3210 2007 1037 10004 3618 2084 2030 5020 2000 1037 2373 4425 10004 1010 1996 2034 4664 1997 1996 29018 1997 103 3210 2164 1037 2034 2773 2240 5516 2000 1996 5164 7276 2240 1998 1037 2117 2773 2240 5516 2000 1996 2598 7276 2240 1025 12515 3251 11207 10004 2015 1997 1996 3638 4442 2031 2584 1037 4539 11207 10004 1025 103 15192 2012 2560 1996 2034 4439 1010 1996 2117 4439 1998 1996 12515 1037 2193 1997 2335 26199 1996 11207 10004 1997 2169 1997 1996 3638 4442 2003 2896 2084 1996 4539 11207 10004 2065 103 11207 10004 1997 2012 2560 2028 1997 1996 3638 4442 2003 4340 2000 2022 3618 2084 1996 4539 11207 10004 1012 1037 4800 10010 16252 23820 14709 9605 1024 1037 16023 2930 2008 20487 19818 2951 2046 2152 1011 3737 19818 2951 9034 2204 3737 1998 103 19818 2951 2060 2084 2056 2152 1011 3737 19818 2951 1025 1037 4942 10010 16252 16169 2930 103 17751 24388 2229 2056 2152 1011 3737 19818 2951 1998 2056 103 19818 2951 2107 2008 2056 2152 1011 3737 19818 2951 2003 11095 2000 4942 10010 16252 2015 1999 1037 9884 1997 1037 2415 6075 103 1037 3653 3207 3334 25089 6075 5884 1998 2056 6623 19818 2951 2003 11095 2000 4942 10010 16252 2015 1999 1037 9884 1997 2119 4515 1999 1996 3653 3207 3334 25089 6075 5884 1010 1998 2008 9783 1999 10388 2007 3149 3737 1037 2846 1997 4942 10010 16252 2015 2000 2029 2056 2152 1011 3737 19818 2951 2003 11095 1998 1037 2846 102\n",
            "I0811 12:00:07.005140 140302422521728 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:07.005351 140302422521728 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0811 12:00:07.005439 140302422521728 create_pretraining_data.py:161] masked_lm_positions: 39 128 145 156 173 179 191 208 237 266 269 310 328 346 393 404 411 412 423 448\n",
            "I0811 12:00:07.005522 140302422521728 create_pretraining_data.py:161] masked_lm_ids: 3191 1996 5224 1037 22505 2005 1037 5164 2598 1996 2773 1998 2127 1037 6623 1025 2008 4373 6623 1999\n",
            "I0811 12:00:07.005614 140302422521728 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
            "I0811 12:00:07.005707 140302422521728 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0811 12:01:48.070025 140302422521728 create_pretraining_data.py:166] Wrote 151616 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpFcIvb4ghzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "307bcfb3-5db0-4ce8-ccc6-fa319922b94d"
      },
      "source": [
        "# About 350 [MiB]\n",
        "\n",
        "!stat -c %s ../training_data.tfrecord"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "368197714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSmrMZ5_qNy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "1873d9a7-fc61-465d-bf94-6a473082e3a8"
      },
      "source": [
        "!gsutil cp ../training_data.tfrecord gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://../training_data.tfrecord [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/351.1 MiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/351.1 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LntB4_akkNrb",
        "colab_type": "text"
      },
      "source": [
        "## Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77gFndWpk-vR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "66054ff2-6a8c-4e30-d9aa-ba1c249b959d"
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.50.41.114:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 7060222105455313451),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8716928994900460621),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13969103642174635354),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7325847632783373539),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1671794254798757949),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11964686727760805423),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4947439867643438348),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5839443967131183485),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6260677637722179165),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5465426159078997655),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 128610168071181930)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 22:50:50.996374 140083162711936 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTnTutlBv9Zu",
        "colab_type": "text"
      },
      "source": [
        "NOTE: need to give access rights to INIT_CKPT GCS to cloud TPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Kq1GhgoAlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_FILE = \"gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/training_data.tfrecord\"\n",
        "OUTPUT_GCS = \"gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT\"\n",
        "INIT_CKPT = \"gs://yohei-kikuta/mlstudy-phys/bert/models/pre-trained-models/uncased_L-12_H-768_A-12/bert_model.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQCDqxFgj5gq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33182e06-eb85-4f0e-9d68-f79ed05a0a64"
      },
      "source": [
        "%%time\n",
        "\n",
        "!python run_pretraining.py \\\n",
        "  --input_file={INPUT_FILE} \\\n",
        "  --output_dir={OUTPUT_GCS} \\\n",
        "  --use_tpu=True \\\n",
        "  --tpu_name={TPU_ADDRESS} \\\n",
        "  --num_tpu_cores=8 \\\n",
        "  --do_train=True \\\n",
        "  --do_eval=True \\\n",
        "  --bert_config_file=../uncased_L-12_H-768_A-12/bert_config.json \\\n",
        "  --train_batch_size=64 \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --num_train_steps=70000 \\\n",
        "  --num_warmup_steps=1000 \\\n",
        "  --learning_rate=5e-5\n",
        "#  --init_checkpoint={INIT_CKPT} \\  # Only need to add at the first training time."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 03:07:13.807877 140024347654016 deprecation_wrapper.py:119] From /content/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0812 03:07:13.809203 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:493: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0812 03:07:13.810020 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:407: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0812 03:07:13.810220 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:407: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0812 03:07:13.810383 140024347654016 deprecation_wrapper.py:119] From /content/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0812 03:07:13.812720 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:414: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0812 03:07:15.244325 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:418: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0812 03:07:15.479598 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:420: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0812 03:07:15.479886 140024347654016 run_pretraining.py:420] *** Input Files ***\n",
            "I0812 03:07:15.479982 140024347654016 run_pretraining.py:422]   gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/training_data.tfrecord\n",
            "W0812 03:07:16.847363 140024347654016 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0812 03:07:17.854001 140024347654016 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f59bf86e048>) includes params argument, but params are not passed to Estimator.\n",
            "I0812 03:07:17.855996 140024347654016 estimator.py:209] Using config: {'_model_dir': 'gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.50.41.114:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f59bf806390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.50.41.114:8470', '_evaluation_master': 'grpc://10.50.41.114:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f59cbab3dd8>}\n",
            "I0812 03:07:17.856419 140024347654016 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0812 03:07:17.857180 140024347654016 run_pretraining.py:459] ***** Running training *****\n",
            "I0812 03:07:17.857276 140024347654016 run_pretraining.py:460]   Batch size = 64\n",
            "I0812 03:07:21.000814 140024347654016 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.50.41.114:8470) for TPU system metadata.\n",
            "2019-08-12 03:07:21.003401: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0812 03:07:21.018378 140024347654016 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0812 03:07:21.018620 140024347654016 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0812 03:07:21.018716 140024347654016 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0812 03:07:21.018781 140024347654016 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0812 03:07:21.018848 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 7060222105455313451)\n",
            "I0812 03:07:21.019773 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 13969103642174635354)\n",
            "I0812 03:07:21.019855 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7325847632783373539)\n",
            "I0812 03:07:21.019922 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1671794254798757949)\n",
            "I0812 03:07:21.019986 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 11964686727760805423)\n",
            "I0812 03:07:21.020051 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 4947439867643438348)\n",
            "I0812 03:07:21.020133 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 5839443967131183485)\n",
            "I0812 03:07:21.020199 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6260677637722179165)\n",
            "I0812 03:07:21.020259 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5465426159078997655)\n",
            "I0812 03:07:21.020318 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 128610168071181930)\n",
            "I0812 03:07:21.020388 140024347654016 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8716928994900460621)\n",
            "W0812 03:07:21.028681 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0812 03:07:21.043062 140024347654016 estimator.py:1145] Calling model_fn.\n",
            "W0812 03:07:21.043678 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:337: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0812 03:07:21.050139 140024347654016 deprecation.py:323] From run_pretraining.py:368: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0812 03:07:21.050300 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0812 03:07:21.079174 140024347654016 deprecation.py:323] From run_pretraining.py:385: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0812 03:07:21.079432 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0812 03:07:21.081039 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:393: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0812 03:07:21.087328 140024347654016 deprecation.py:323] From run_pretraining.py:400: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "I0812 03:07:21.178682 140024347654016 run_pretraining.py:117] *** Features ***\n",
            "I0812 03:07:21.178993 140024347654016 run_pretraining.py:119]   name = input_ids, shape = (8, 512)\n",
            "I0812 03:07:21.179105 140024347654016 run_pretraining.py:119]   name = input_mask, shape = (8, 512)\n",
            "I0812 03:07:21.179209 140024347654016 run_pretraining.py:119]   name = masked_lm_ids, shape = (8, 20)\n",
            "I0812 03:07:21.179291 140024347654016 run_pretraining.py:119]   name = masked_lm_positions, shape = (8, 20)\n",
            "I0812 03:07:21.179380 140024347654016 run_pretraining.py:119]   name = masked_lm_weights, shape = (8, 20)\n",
            "I0812 03:07:21.179461 140024347654016 run_pretraining.py:119]   name = next_sentence_labels, shape = (8, 1)\n",
            "I0812 03:07:21.179536 140024347654016 run_pretraining.py:119]   name = segment_ids, shape = (8, 512)\n",
            "W0812 03:07:21.179773 140024347654016 deprecation_wrapper.py:119] From /content/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0812 03:07:21.182161 140024347654016 deprecation_wrapper.py:119] From /content/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0812 03:07:21.220245 140024347654016 deprecation_wrapper.py:119] From /content/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0812 03:07:21.405688 140024347654016 deprecation.py:506] From /content/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0812 03:07:21.427104 140024347654016 deprecation.py:323] From /content/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "I0812 03:07:25.621255 140024347654016 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0812 03:07:25.621549 140024347654016 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768)\n",
            "I0812 03:07:25.621692 140024347654016 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0812 03:07:25.621784 140024347654016 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0812 03:07:25.621876 140024347654016 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.621960 140024347654016 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.622043 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.622140 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.622224 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.622308 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.622416 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.622497 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.622573 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.622651 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.622726 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.622800 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.622874 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.622953 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.623028 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.623105 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.623196 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.623270 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.623352 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.623430 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.623505 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.623581 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.623654 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.623732 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.623806 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.623885 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.623959 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.624032 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.624106 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.624198 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.624273 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.624359 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.624435 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.624508 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.624582 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.624659 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.624733 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.624818 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.624907 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.625002 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.625084 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.625181 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.625257 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.625331 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.625413 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.625493 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.625567 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.625644 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.625720 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.625792 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.625868 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.625947 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.626022 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.626098 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.626187 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.626265 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.626345 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.626425 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.626499 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.626572 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.626645 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.626721 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.626795 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.626873 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.626949 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.627021 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.627094 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.627185 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.627259 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.627343 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.627417 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.627494 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.627568 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.627645 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.627720 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.627793 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.627869 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.627946 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.628021 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.628098 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.628186 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.628260 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.628340 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.628417 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.628490 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.628571 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.628646 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.628723 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.628800 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.628879 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.628953 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.629027 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.629100 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.629191 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.629266 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.629349 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.629424 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.629499 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.629572 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.629648 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.629724 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.629801 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.629875 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.629952 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.630026 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.630103 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.630194 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.630268 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.630347 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.630426 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.630501 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.630578 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.630653 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.630727 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.630800 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.630878 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.630953 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.631031 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.631105 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.675735 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.676073 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.676257 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.676413 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.676522 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.676630 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.676740 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.676847 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.676957 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.677059 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.677167 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.677270 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.677396 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.677507 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.677609 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.677714 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.677826 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.677935 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.678045 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.678170 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.678283 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.678400 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.678511 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.678619 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.678732 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.678838 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.678941 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.679056 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.679186 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.679293 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.679417 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.679529 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.679639 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.679743 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.679853 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.679962 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.680064 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.680212 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.680331 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.680451 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.680564 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.680673 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.680776 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.680880 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.680989 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.681098 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.681232 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.681348 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.681467 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.681575 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.681686 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.681795 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.681902 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.682005 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.682131 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.682245 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.682366 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.682474 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.682579 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.682684 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.682790 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:07:25.682896 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.683008 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:07:25.683131 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.683246 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:07:25.683362 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.683475 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.683579 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.683681 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.683787 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:07:25.683897 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:07:25.684000 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:07:25.684127 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.684239 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.684350 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.684455 140024347654016 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.684566 140024347654016 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.684672 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:07:25.684781 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0812 03:07:25.684888 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:07:25.684994 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:07:25.685095 140024347654016 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30522,)\n",
            "I0812 03:07:25.685220 140024347654016 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0812 03:07:25.685332 140024347654016 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0812 03:07:25.685564 140024347654016 deprecation_wrapper.py:119] From /content/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0812 03:07:25.687593 140024347654016 deprecation_wrapper.py:119] From /content/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n",
            "W0812 03:07:25.696297 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0812 03:07:25.995589 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "I0812 03:07:40.786602 140024347654016 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0812 03:07:41.493098 140024347654016 estimator.py:1147] Done calling model_fn.\n",
            "I0812 03:07:46.003062 140024347654016 tpu_estimator.py:499] TPU job name worker\n",
            "I0812 03:07:47.441536 140024347654016 monitored_session.py:240] Graph was finalized.\n",
            "W0812 03:07:47.639671 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0812 03:07:47.848248 140024347654016 saver.py:1280] Restoring parameters from gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/model.ckpt-69000\n",
            "W0812 03:08:21.574569 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0812 03:08:25.054666 140024347654016 session_manager.py:500] Running local_init_op.\n",
            "I0812 03:08:26.158855 140024347654016 session_manager.py:502] Done running local_init_op.\n",
            "I0812 03:08:39.488061 140024347654016 basic_session_run_hooks.py:606] Saving checkpoints for 69000 into gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/model.ckpt.\n",
            "W0812 03:09:14.333646 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:741: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "I0812 03:09:16.602204 140024347654016 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0812 03:09:16.603595 140024347654016 session_support.py:332] Installing graceful shutdown hook.\n",
            "2019-08-12 03:09:16.604751: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:356] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "I0812 03:09:16.610590 140024347654016 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0812 03:09:16.614392 140024347654016 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0812 03:09:16.619072 140024347654016 tpu_estimator.py:557] Init TPU system\n",
            "I0812 03:09:21.248434 140024347654016 tpu_estimator.py:566] Initialized TPU in 4 seconds\n",
            "I0812 03:09:21.249644 140023199643392 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0812 03:09:21.250196 140023173428992 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0812 03:09:22.366356 140024347654016 tpu_estimator.py:590] Enqueue next (1000) batch(es) of data to infeed.\n",
            "I0812 03:09:22.367506 140024347654016 tpu_estimator.py:594] Dequeue next (1000) batch(es) of data from outfeed.\n",
            "I0812 03:09:55.457108 140023173428992 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0812 03:10:55.556624 140023173428992 tpu_estimator.py:275] Outfeed finished for iteration (0, 252)\n",
            "I0812 03:11:55.658803 140023173428992 tpu_estimator.py:275] Outfeed finished for iteration (0, 504)\n",
            "I0812 03:12:55.762263 140023173428992 tpu_estimator.py:275] Outfeed finished for iteration (0, 756)\n",
            "I0812 03:13:54.606079 140024347654016 basic_session_run_hooks.py:606] Saving checkpoints for 70000 into gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/model.ckpt.\n",
            "W0812 03:14:24.131874 140024347654016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "I0812 03:14:30.867539 140024347654016 basic_session_run_hooks.py:262] loss = 0.01433176, step = 70000\n",
            "I0812 03:14:31.933023 140024347654016 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0812 03:14:31.933333 140024347654016 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0812 03:14:31.933667 140023199643392 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0812 03:14:31.933796 140023199643392 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0812 03:14:31.933971 140024347654016 error_handling.py:96] infeed marked as finished\n",
            "I0812 03:14:31.934183 140024347654016 tpu_estimator.py:602] Stop output thread controller\n",
            "I0812 03:14:31.934273 140024347654016 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0812 03:14:31.934426 140023173428992 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0812 03:14:31.934525 140023173428992 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0812 03:14:31.934644 140024347654016 error_handling.py:96] outfeed marked as finished\n",
            "I0812 03:14:31.934724 140024347654016 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0812 03:14:33.690581 140024347654016 estimator.py:368] Loss for final step: 0.01433176.\n",
            "I0812 03:14:33.692697 140024347654016 error_handling.py:96] training_loop marked as finished\n",
            "I0812 03:14:33.693103 140024347654016 run_pretraining.py:469] ***** Running evaluation *****\n",
            "I0812 03:14:33.693264 140024347654016 run_pretraining.py:470]   Batch size = 8\n",
            "I0812 03:14:34.580273 140024347654016 estimator.py:1145] Calling model_fn.\n",
            "I0812 03:14:34.688871 140024347654016 run_pretraining.py:117] *** Features ***\n",
            "I0812 03:14:34.689178 140024347654016 run_pretraining.py:119]   name = input_ids, shape = (1, 512)\n",
            "I0812 03:14:34.689274 140024347654016 run_pretraining.py:119]   name = input_mask, shape = (1, 512)\n",
            "I0812 03:14:34.689350 140024347654016 run_pretraining.py:119]   name = masked_lm_ids, shape = (1, 20)\n",
            "I0812 03:14:34.689416 140024347654016 run_pretraining.py:119]   name = masked_lm_positions, shape = (1, 20)\n",
            "I0812 03:14:34.689502 140024347654016 run_pretraining.py:119]   name = masked_lm_weights, shape = (1, 20)\n",
            "I0812 03:14:34.689565 140024347654016 run_pretraining.py:119]   name = next_sentence_labels, shape = (1, 1)\n",
            "I0812 03:14:34.689649 140024347654016 run_pretraining.py:119]   name = segment_ids, shape = (1, 512)\n",
            "I0812 03:14:38.879699 140024347654016 run_pretraining.py:167] **** Trainable Variables ****\n",
            "I0812 03:14:38.879982 140024347654016 run_pretraining.py:173]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768)\n",
            "I0812 03:14:38.880130 140024347654016 run_pretraining.py:173]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768)\n",
            "I0812 03:14:38.880224 140024347654016 run_pretraining.py:173]   name = bert/embeddings/position_embeddings:0, shape = (512, 768)\n",
            "I0812 03:14:38.880319 140024347654016 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.880410 140024347654016 run_pretraining.py:173]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.880492 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.880575 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.880650 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.880730 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.880805 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.880883 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.880959 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.881037 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.881125 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.881202 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.881277 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.881361 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.881437 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.881515 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.881590 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.881664 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.881737 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.881815 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.881892 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.881976 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.882051 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.882168 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.882280 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.882362 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.882445 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.882519 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.882592 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.882672 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.882746 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.882822 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.882907 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.882980 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.883053 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.883143 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.883218 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.883296 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.883369 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.883445 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.883517 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.883592 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.883664 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.883737 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.883810 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.883892 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.883967 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.884042 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.884130 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.884206 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.884278 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.884355 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.884428 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.884506 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.884579 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.884654 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.884727 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.884803 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.884881 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.884953 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.885025 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.885100 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.885187 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.885265 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.885338 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.885412 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.885485 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.885561 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.885633 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.885711 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.885784 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.885864 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.885937 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.886011 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.886082 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.886175 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.886249 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.886324 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.886399 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.886475 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.886548 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.886620 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.886691 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.886767 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.886840 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.886922 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.886996 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.887072 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.887158 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.887236 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.887307 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.887381 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.887452 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.887528 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.887602 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.887678 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.887749 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.887820 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.887898 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.887973 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.888046 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.888135 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.888214 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.888290 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.888363 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.888440 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.888513 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.888585 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.888657 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.888732 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.888803 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.888883 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.888957 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.889029 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.889101 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.889191 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.889264 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.889338 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.889415 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.889489 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.960630 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.960970 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.961181 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.961316 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.961464 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.961597 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.961728 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.961854 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.961959 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.962067 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.962207 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.962324 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.962455 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.962568 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.962685 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.962798 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.962907 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.963020 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.963145 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.963260 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.963382 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.963497 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.963605 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.963720 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.963826 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.963931 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.964036 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.964163 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.964275 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.964401 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.964510 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.964623 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.964727 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.964837 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.964941 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.965045 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.965168 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.965273 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.965381 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.965489 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.965589 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.965689 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.965786 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.965889 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.965989 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.966093 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.966220 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.966329 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.966446 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.966570 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.966675 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.966780 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.966886 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.966997 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.967103 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.967243 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.967348 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.967470 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.967577 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.967686 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,)\n",
            "I0812 03:14:38.967791 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.967903 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,)\n",
            "I0812 03:14:38.968008 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.968147 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,)\n",
            "I0812 03:14:38.968265 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.968388 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.968497 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.968604 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.968708 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072)\n",
            "I0812 03:14:38.968819 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,)\n",
            "I0812 03:14:38.968925 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768)\n",
            "I0812 03:14:38.969042 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.969171 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.969283 140024347654016 run_pretraining.py:173]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.969403 140024347654016 run_pretraining.py:173]   name = bert/pooler/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.969518 140024347654016 run_pretraining.py:173]   name = bert/pooler/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.969626 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/dense/kernel:0, shape = (768, 768)\n",
            "I0812 03:14:38.969735 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/dense/bias:0, shape = (768,)\n",
            "I0812 03:14:38.969841 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/beta:0, shape = (768,)\n",
            "I0812 03:14:38.969949 140024347654016 run_pretraining.py:173]   name = cls/predictions/transform/LayerNorm/gamma:0, shape = (768,)\n",
            "I0812 03:14:38.970051 140024347654016 run_pretraining.py:173]   name = cls/predictions/output_bias:0, shape = (30522,)\n",
            "I0812 03:14:38.970187 140024347654016 run_pretraining.py:173]   name = cls/seq_relationship/output_weights:0, shape = (2, 768)\n",
            "I0812 03:14:38.970306 140024347654016 run_pretraining.py:173]   name = cls/seq_relationship/output_bias:0, shape = (2,)\n",
            "W0812 03:14:39.394546 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:198: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
            "\n",
            "W0812 03:14:39.412211 140024347654016 deprecation_wrapper.py:119] From run_pretraining.py:202: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "I0812 03:14:40.469680 140024347654016 estimator.py:1147] Done calling model_fn.\n",
            "I0812 03:14:40.491405 140024347654016 evaluation.py:255] Starting evaluation at 2019-08-12T03:14:40Z\n",
            "I0812 03:14:40.491708 140024347654016 tpu_estimator.py:499] TPU job name worker\n",
            "I0812 03:14:41.048415 140024347654016 monitored_session.py:240] Graph was finalized.\n",
            "I0812 03:14:41.248930 140024347654016 saver.py:1280] Restoring parameters from gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/model.ckpt-70000\n",
            "I0812 03:15:10.822107 140024347654016 session_manager.py:500] Running local_init_op.\n",
            "I0812 03:15:11.011430 140024347654016 session_manager.py:502] Done running local_init_op.\n",
            "I0812 03:15:11.467064 140024347654016 tpu_estimator.py:557] Init TPU system\n",
            "I0812 03:15:20.132824 140024347654016 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0812 03:15:20.133843 140023382632192 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0812 03:15:20.134232 140023366117120 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0812 03:15:20.356702 140024347654016 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0812 03:15:20.537091 140024347654016 tpu_estimator.py:590] Enqueue next (100) batch(es) of data to infeed.\n",
            "I0812 03:15:20.537555 140024347654016 tpu_estimator.py:594] Dequeue next (100) batch(es) of data from outfeed.\n",
            "I0812 03:15:26.606899 140023366117120 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0812 03:15:27.935293 140024347654016 evaluation.py:167] Evaluation [100/100]\n",
            "I0812 03:15:27.935819 140024347654016 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0812 03:15:27.935910 140024347654016 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0812 03:15:27.936213 140023382632192 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0812 03:15:27.936519 140023382632192 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0812 03:15:27.936754 140024347654016 error_handling.py:96] infeed marked as finished\n",
            "I0812 03:15:27.936954 140024347654016 tpu_estimator.py:602] Stop output thread controller\n",
            "I0812 03:15:27.937018 140024347654016 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0812 03:15:28.640151 140023366117120 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0812 03:15:28.640423 140023366117120 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0812 03:15:28.640692 140024347654016 error_handling.py:96] outfeed marked as finished\n",
            "I0812 03:15:28.640969 140024347654016 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0812 03:15:29.288392 140024347654016 evaluation.py:275] Finished evaluation at 2019-08-12-03:15:29\n",
            "I0812 03:15:29.288794 140024347654016 estimator.py:2039] Saving dict for global step 70000: global_step = 70000, loss = 0.0005139347, masked_lm_accuracy = 0.9999375, masked_lm_loss = 0.00031679025, next_sentence_accuracy = 1.0, next_sentence_loss = 1.9996969e-07\n",
            "I0812 03:15:33.362672 140024347654016 estimator.py:2099] Saving 'checkpoint_path' summary for global step 70000: gs://yohei-kikuta/mlstudy-phys/patent-analysis/3000-pretrain-BERT/model.ckpt-70000\n",
            "I0812 03:15:34.621418 140024347654016 error_handling.py:96] evaluation_loop marked as finished\n",
            "I0812 03:15:34.621796 140024347654016 run_pretraining.py:483] ***** Eval results *****\n",
            "I0812 03:15:34.621907 140024347654016 run_pretraining.py:485]   global_step = 70000\n",
            "I0812 03:15:34.622360 140024347654016 run_pretraining.py:485]   loss = 0.0005139347\n",
            "I0812 03:15:34.622489 140024347654016 run_pretraining.py:485]   masked_lm_accuracy = 0.9999375\n",
            "I0812 03:15:34.622562 140024347654016 run_pretraining.py:485]   masked_lm_loss = 0.00031679025\n",
            "I0812 03:15:34.622630 140024347654016 run_pretraining.py:485]   next_sentence_accuracy = 1.0\n",
            "I0812 03:15:34.622724 140024347654016 run_pretraining.py:485]   next_sentence_loss = 1.9996969e-07\n",
            "CPU times: user 2.61 s, sys: 373 ms, total: 2.99 s\n",
            "Wall time: 8min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lfxg-o6vLR0",
        "colab_type": "text"
      },
      "source": [
        "## Memo\n",
        "\n",
        "```\n",
        "I0811 12:50:14.190979 140514602837888 run_pretraining.py:483] ***** Eval results *****\n",
        "I0811 12:50:14.191113 140514602837888 run_pretraining.py:485]   global_step = 20\n",
        "I0811 12:50:14.191493 140514602837888 run_pretraining.py:485]   loss = 1.0284224\n",
        "I0811 12:50:14.191617 140514602837888 run_pretraining.py:485]   masked_lm_accuracy = 0.8165625\n",
        "I0811 12:50:14.191753 140514602837888 run_pretraining.py:485]   masked_lm_loss = 0.86381125\n",
        "I0811 12:50:14.191835 140514602837888 run_pretraining.py:485]   next_sentence_accuracy = 0.95\n",
        "I0811 12:50:14.191914 140514602837888 run_pretraining.py:485]   next_sentence_loss = 0.1793103\n",
        "CPU times: user 1.56 s, sys: 244 ms, total: 1.8 s\n",
        "Wall time: 4min 17s\n",
        "```\n",
        "\n",
        "```\n",
        "I0811 14:17:34.821915 139717559273344 run_pretraining.py:483] ***** Eval results *****\n",
        "I0811 14:17:34.822047 139717559273344 run_pretraining.py:485]   global_step = 15000\n",
        "I0811 14:17:34.822362 139717559273344 run_pretraining.py:485]   loss = 0.18834595\n",
        "I0811 14:17:34.822492 139717559273344 run_pretraining.py:485]   masked_lm_accuracy = 0.9595625\n",
        "I0811 14:17:34.822575 139717559273344 run_pretraining.py:485]   masked_lm_loss = 0.14227556\n",
        "I0811 14:17:34.822673 139717559273344 run_pretraining.py:485]   next_sentence_accuracy = 0.99875\n",
        "I0811 14:17:34.822756 139717559273344 run_pretraining.py:485]   next_sentence_loss = 0.0048195585\n",
        "```\n",
        "\n",
        "```\n",
        "I0812 03:15:34.621796 140024347654016 run_pretraining.py:483] ***** Eval results *****\n",
        "I0812 03:15:34.621907 140024347654016 run_pretraining.py:485]   global_step = 70000\n",
        "I0812 03:15:34.622360 140024347654016 run_pretraining.py:485]   loss = 0.0005139347\n",
        "I0812 03:15:34.622489 140024347654016 run_pretraining.py:485]   masked_lm_accuracy = 0.9999375\n",
        "I0812 03:15:34.622562 140024347654016 run_pretraining.py:485]   masked_lm_loss = 0.00031679025\n",
        "I0812 03:15:34.622630 140024347654016 run_pretraining.py:485]   next_sentence_accuracy = 1.0\n",
        "I0812 03:15:34.622724 140024347654016 run_pretraining.py:485]   next_sentence_loss = 1.9996969e-07\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp9YOWPzk6vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}